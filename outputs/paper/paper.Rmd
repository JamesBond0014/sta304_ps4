---
title: "Trump Expecting to Win 53% of Popular Vote in 2020 US Presidential Election"
subtitle: "4% Confidence Interval Based on a Survey from July 2020"
author: "TBD"
date: "`r format(Sys.time(), '%d %B %Y')`"
thanks: "Code and data supporting this analysis are available at: https://github.com/JamesBond0014/sta304_ps4."
abstract: |
  | First sentence, second sentence, third sentence, fourth sentence
  |
  | **Keywords:** forecasting, US 2020 Election, Trump, Biden, multilevel regression with post-stratification

output:
  bookdown::pdf_document2:
toc: FALSE
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(palmerpenguins)
```

```{r}
#load required data and models

cleaned_data <- data.frame(read.csv(file = "../../inputs/cleaned_acs.csv"))

```

# Abstract
# Introduction
# Data
To train our model to predict the outcome of the 2020 US presidential election, we used Wave 49 of the Nationscape Dataset (results from the week of June 18-24, 2020). We will discuss how this data was collected, its key features, and what the data looks like in the section titled "Individual-level survey dataset".

TODO: To make predictions on the outcome of the 2020 US presidential election, we used... We will discuss how this data was collected, its key features, and what the data looks like in the section titled "Post-stratification dataset". The explanation of multilevel modelling with post-stratification can be found in the "Model" section.

## Individual-level survey dataset
The Nationscape Project is 16-month-long voter study (from July 2019 to January 2021) that conducts weekly surveys regarding the 2020 US presidential election. We will solely discuss Wave 49 of the Nationscape Dataset for the reminder of this paper.

From June 18, 2020 to June 24, 2020, Nationscape collected data on public opinion about the 2020 presidential campaign and election by conducting online interviews. Their target is the American "population". Unfortunately, the published information on their methodology is not more specific as to what constitutes a member of the American population (are they interested in the opinions of noncitizens?). Presumably, their target population is all individuals presently residing in the United States. Nationscape uses the audience of market research platform Lucid as its sampling frame, i.e., the survey respondents on Lucid are the frame for this dataset. Finally, a sample matching the demographics of the American population is selected from the frame. Unfortunately, further details on their sampling methodology were not provided: potentially they could have conducted simple random stratified sampling or picked random samples from the sampling frame (passing on individuals if quotas for their demographic were met). After being contacted by Lucid to take the survey, respondents are immediately redirected to Nationscape survey software where the questionnaire starts.


-methodology and approach processing the data
-non response

-key features, 
-strengths
-weaknesses
-discuss variables you use
-similar
-combinining?
-plot data
-discuss plots 

## Post-stratification dataset
The post-stratification dataset was gathered from the American Community Survey (ACS), a project aiming to mitigate issues stemming from the census’ 10-year intervals by providing an annualized version of data like that produced by the decennial census long form. The dataset used in this study is specifically that of the 2018 ACS data. The ACS data can be accessed at the IPUMS website. More details on attaining and cleaning data are found in 01-data_cleaning-post-strat.R in the scripts folder of the git repository.

The target population, much like the census, is essentially anyone who resides in a dwelling in the US. Following this, the sampling frame of the ACS is the Master Address File that is maintained by the US Census Bureau. Created for the 2000 Census, it was originally based on the 1990 Address Control File and the United States Postal Service’s Delivery Sequence File. The maintaining and updating of this file is crucial to the efforts of the ACS and any other body that makes use of it. In addition, the ACS samples 2.5 percent of the population living “Group Quarters”, non-housing units (eg. nursing homes, prisons, college dorms, etc.). In total, the 2018 ACS data contains about 3.2 million observations, sampled from across the country.

Every month, a systemic sample is created for each US county or equivalent, where they are mailed the ACS survey at the start of the month. As of February 2002, the sampling rate of all counties has been 2.5%, except for Houston, Texas, which is sampled at 1% (due to the size of the population). For every site, the sampling is broken into two steps. The first step is sampling 17.5% of the population, which is then subsampled from to achieve that desired percentage. All non-respondents are subsequently contacted by phone for a computer assisted telephone interview one month later. One third of non-respondents that have reached this point are then sampled from to be contacted for a computer assisted personal interview following the previous telephone interview attempt. Beyond this sample (referred to as the National Sample or Supplemental Sample), data was also collected at 31 selected test sites to represent areas with various county population sizes or areas that were difficult to enumerate. The ACS data is weighted in order to ensure reliable and usable estimated regarding the population. 

The ACS Questionnaire asks questions regarding every inhabitant in the residence it is sent to. However, the most information is required of “Person 1”, the person whose name the residence is owned in, being bought in, or rented in (or any adult, if none of those labels apply). The ACS questionnaire is extremely like that of the US Census Questionnaire, given that it’s meant to be a substitution for it. It was developed after the Census Bureau was provided with various subjects that other federal agencies justified as important, categorizing each subject as “mandatory”, “required”, or “programmatic”, from which the ACS collected data for both the “mandatory” and “required” subjects. This approach illustrates itself in the questionnaire, as every question is answered by an objective fact, whether it’s a checkbox, a number, or a short answer (such as the person’s major). This allows little ambiguity in how the question must be answered, and makes each response fairly easy, which is a major strength of the questionnaire. However, depending on the number of people in the residence, the survey can be on the longer side (up to 11 pages). Combined with the fact that it is legally mandated that the response is filled out, with a potential fine of up to $5000, respondents may feel the need to rush through or give a fake response to questions they may not know immediately (such as when the residence was built). In addition, in the case that there is not one single person responsible for the residence, like in the case where multiple people are renting a room from one landlord who doesn’t live at the residence, then the data of those who are not “Person 1” is not collected, despite the fact that they are on the same standing in the eyes of the survey. However, no one has been prosecuted for not filling out the survey since 1970 and the case outlined in the second point is relatively rare in the total population, so these weaknesses are fairly minor in the grand scheme of things. 

As mentioned above, the variables we used were age (the age of the respondent), state (the state they lived in), gender (what gender they identified as), education (what their highest level of education was), and race/ethnicity of the respondent. However, as the post-stratification data was a different dataset to that of the survey data, we had to transform the post-stratification data to match that, including general cleaning in the sense of converting non-numeric values to numeric, matching up spelling/capitalization of certain responses, and constructing an age_groups variable from the ages given. For more information, please see 01-data_cleaning-post-strat.R in the scripts folder of the github repository.   <-- This section is to be updated accordingly

```{r post-strat graphs}
# Plot Race
perc_race <- cleaned_data %>% count(race_ethnicity) %>% mutate(perc = n/nrow(cleaned_data))
race <- perc_race %>% ggplot(aes(x = race_ethnicity, y = perc)) + geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + labs(title = "Race/Ethnicity of Respondents in 2018 ACS data",
                                                                               x = "Race/Ethnicity", y = "Percentage", subtitle = "Figure X")
race

# Plot Gender
perc_gender <- cleaned_data %>% count(gender) %>% mutate(perc = n/nrow(cleaned_data))
gender <- perc_gender %>% ggplot(aes(x = gender, y = perc)) + geom_bar(stat = "identity") + 
  labs(title = "Gender of Respondents in 2018 ACS data", x = "Gender", y = "Percentage", subtitle = "Figure X")
gender

# Plot education
perc_education <- cleaned_data %>% count(education) %>% mutate(perc = n/nrow(cleaned_data))
perc_education$education <- perc_education$education %>% factor(levels = c("High School or less", "Post Secondary Degree"))
education <- perc_education %>% ggplot(aes(x = education, y = perc)) + geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + labs(title = "Education of Respondents in 2018 ACS data",
                                                                               x = "Education", y = "Percentage", subtitle = "Figure X")
summary(cleaned_data$education)
education

# Explain 88 Olds are shit


# Plot state
perc_state <- cleaned_data %>% count(state) %>% mutate(perc = n/nrow(cleaned_data))
perc_state$state <- perc_state$state %>% factor(levels = sort(as.character.factor(perc_state$state))) 
state <- perc_state %>% ggplot(aes(x = state, y = perc)) + geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + labs(title = "State of Respondents in 2018 ACS data",
                                                                               x = "State", y = "Percentage", subtitle = "Figure X")
state

# Plot age_group
perc_age_group <- cleaned_data %>% count(age_group) %>% mutate(perc = n/nrow(cleaned_data))
#perc_age_group$age_group <- perc_age_group$age_group %>% factor(levels = sort(as.character.factor(perc_age_group$age_group))) 
age_group <- perc_age_group %>% ggplot(aes(x = age_group, y = perc)) + geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + labs(title = "Age of Respondents in 2018 ACS data",
                                                                               x = "Age Group", y = "Percentage", subtitle = "Figure X")
age_group
```


# Model

\begin{equation}
Pr(\theta | y) = \frac{Pr(y | \theta) Pr(\theta)}{Pr(y)}  (\#eq:bayes)
\end{equation}

Since our predictor y is a binary variable (Biden or Trump), we used a logistic regression in order build our model. A logistic regression predicts the likelihood of an outcome based on various independent variables and it fits a model on a log curve which is bound in the Y axis in the range of [0,1].. In our case, we judge the likelihood that an individual would vote for Joe Biden in comparison to Donald Trump, given the individuals age, gender, education, state, and race. Given these inputs and outputs, we can train our logistic regression model using BRMS to predict which values for each variable predicts certain votes, and this will be discussed further in the later section. In our model, as it is a binary predictor, we round the prediction to the nearest number (more than 0.5 is rounded to 1, meaning this individual is likely to vote for Joe Biden, and less than 0.5 is rounded to 0, meaning this individual is likely to vote to Donald Trump).

One variable we discussed about including was annual income, however when we added the income lowered our cross validation score and the well discussed issues with self-reported income (bias, inaccurate estimations, not reporting) convinced us to disclude income. Another variable we debated on was education. Originally, education was included, however, education severely impacted our cross validation accuracy. We solved these issues by summarizing all education levels into 2 levels: at least some Post-Secondary and No post-secondary. This allowed our model to perform slightly better, but more importantly, there was a significant difference in the voting preference between the 2 groups. Another variable that helped increase accuracy after manipulation was age, where it became a better independent variable after breaking age into groups on 10's (eg. 18-28, 29-38) in order to better represent different age groups. Other groupings we tried was breaking into youth, middle age, and seniors (18-35, 36-55, 55+) and leaving age as individual numeric values, however both of these breakdowns did not provide any significant value. 

Overall, our model performs fairly (Figure N), with a cross-validation accuracy of approximately 63% (Figure N). 

```{r}
model_education <- readRDS("../../model/4chains_3000iter_.rds")

reduced_data <- readRDS("../../inputs/training_data.Rda")


total_rows <- nrow(reduced_data)

# get updated columns (used for the model)
reduced_data <- reduced_data %>% select(vote_biden,age_group, education, gender,
                                        race_ethnicity, state)

set.seed(50)
#set
shuffle_indices <- sample(total_rows)
data <- reduced_data[shuffle_indices,]
boundary <- as.integer(total_rows*0.95)

training <- data[0:boundary,]

testing <- data[boundary:total_rows,]

#testing the accuracy of the model
probability <- predict(model_education, type = "response", newdata = testing)
probability <- if_else(probability[,1] >0.5,1,0)
testing$probs <- probability
testing <- testing %>% mutate(acc = probs==vote_biden)
data.frame(table(testing$acc)) %>% 
  ggplot(aes(x = Var1, y = Freq/(sum(Freq)))) +
  geom_bar(stat="identity") +
  xlab("Correct Prediction") +
  ylab("Percent") +
  ggtitle("Cross Validation Scores")

```

Equation \@ref(eq:bayes) seems useful, eh?

Here's a dumb example of how to use some references: In paper we run our analysis in `R` [@citeR]. We also use the `tidyverse` which was written by @thereferencecanbewhatever If we were interested in baseball data then @citeLahman could be useful. 
@tausanovitch_vavreck_2019

# Results
Our data is of penguins (Figure \@ref(fig:bills)).

```{r bills, fig.cap="Bills of penguins", echo = FALSE}
ggplot(penguins, aes(x = island, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange","purple","cyan4"),
                    guide = FALSE) +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()
```

Talk more about it.

Also bills and their average (Figure \@ref(fig:billssssss)). (Notice how you can change the height and width so they don't take the whole page?)

```{r billssssss, fig.cap="More bills of penguins", echo = FALSE, fig.width=8, fig.height=4}

# This needs to be about the random data tha tI sasmpled?
ggplot(penguins, aes(x = island, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange","purple","cyan4"),
                    guide = FALSE) +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()
```

Talk way more about it. 
# Discussion

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

# Appendix {-}

\newpage


# References
R
Survey and ACS dataset
https://www.voterstudygroup.org/uploads/reports/Data/Nationscape-User-Guide_2020sep10.pdf
https://www.voterstudygroup.org/uploads/reports/Data/NS-Methodology-Representativeness-Assessment.pdf

Tausanovitch, Chris and Lynn Vavreck. 2020. Democracy Fund + UCLA Nationscape, October 10-17, 2019 (version 20200814). Retrieved from [URL].

