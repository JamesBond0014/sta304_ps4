---
title: "Trump Expecting to Win 53% of Popular Vote in 2020 US Presidential Election"
subtitle: "4% Confidence Interval Based on a Survey from July 2020"
author: "TBD"
date: "`r format(Sys.time(), '%d %B %Y')`"
thanks: "Code and data supporting this analysis are available at: https://github.com/JamesBond0014/sta304_ps4."
abstract: |
  | First sentence, second sentence, third sentence, fourth sentence
  |
  | **Keywords:** forecasting, US 2020 Election, Trump, Biden, multilevel regression with post-stratification

output:
  bookdown::pdf_document2:
toc: FALSE
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(palmerpenguins)
```

```{r}
#load required data and models

cleaned_data <- data.frame(read.csv(file = "../../inputs/cleaned_acs.csv"))

```

# Abstract
# Introduction
# Data
To train our model to predict the outcome of the 2020 US presidential election, we used Wave 49 of the Nationscape Dataset (results from the week of June 18-24, 2020). We will discuss how this data was collected, its key features, and what the data looks like in the section titled "Individual-level survey dataset".

TODO: To make predictions on the outcome of the 2020 US presidential election, we used... We will discuss how this data was collected, its key features, and what the data looks like in the section titled "Post-stratification dataset". The explanation of multilevel modelling with post-stratification can be found in the "Model" section.

## Individual-level survey dataset
The Nationscape Project is 16-month-long voter study (from July 2019 to January 2021) that conducts weekly surveys regarding the 2020 US presidential election. We will solely discuss Wave 49 of the Nationscape Dataset for the reminder of this paper.

From June 18, 2020 to June 24, 2020, Nationscape collected data on public opinion about the 2020 presidential campaign and election by conducting online interviews. Their target is the American "population". Unfortunately, the published information on their methodology is not more specific as to what constitutes a member of the American population (are they interested in the opinions of noncitizens?). Presumably, their target population is all individuals presently residing in the United States. Nationscape uses the audience of market research platform Lucid as its sampling frame, i.e., the survey respondents on Lucid are the frame for this dataset. Finally, a sample matching the demographics of the American population is selected from the frame. Unfortunately, further details on their sampling methodology were not provided: potentially they could have conducted simple random stratified sampling or picked random samples from the sampling frame (passing on individuals if quotas for their demographic were met). After being contacted by Lucid to take the survey, respondents are immediately redirected to Nationscape survey software where the questionnaire starts.


-methodology and approach processing the data
-non response

-key features, 
-strengths
-weaknesses
-discuss variables you use
-similar
-combinining?
-plot data
-discuss plots 

## Post-stratification dataset
The post-stratification dataset was gathered from the American Community Survey (ACS), a project aiming to mitigate issues stemming from the census’ 10-year intervals by providing an annualized version of data like that produced by the decennial census long form. The dataset used in this study is specifically that of the 2018 ACS data. The ACS data can be accessed at the IPUMS website. More details on attaining and cleaning data are found in 01-data_cleaning-post-strat.R in the scripts folder of the git repository.

The target population, much like the census, is essentially anyone who resides in a dwelling in the US. Following this, the sampling frame of the ACS is the Master Address File that is maintained by the US Census Bureau. Created for the 2000 Census, it was originally based on the 1990 Address Control File and the United States Postal Service’s Delivery Sequence File. The maintaining and updating of this file is crucial to the efforts of the ACS and any other body that makes use of it. In addition, the ACS samples 2.5 percent of the population living “Group Quarters”, non-housing units (eg. nursing homes, prisons, college dorms, etc.). In total, the 2018 ACS data contains about 3.2 million observations, sampled from across the country.

Every month, a systemic sample is created for each US county or equivalent, where they are mailed the ACS survey at the start of the month. As of February 2002, the sampling rate of all counties has been 2.5%, except for Houston, Texas, which is sampled at 1% (due to the size of the population). For every site, the sampling is broken into two steps. The first step is sampling 17.5% of the population, which is then subsampled from to achieve that desired percentage. All non-respondents are subsequently contacted by phone for a computer assisted telephone interview one month later. One third of non-respondents that have reached this point are then sampled from to be contacted for a computer assisted personal interview following the previous telephone interview attempt. Beyond this sample (referred to as the National Sample or Supplemental Sample), data was also collected at 31 selected test sites to represent areas with various county population sizes or areas that were difficult to enumerate. The ACS data is weighted in order to ensure reliable and usable estimated regarding the population. 

The ACS Questionnaire asks questions regarding every inhabitant in the residence it is sent to. However, the most information is required of “Person 1”, the person whose name the residence is owned in, being bought in, or rented in (or any adult, if none of those labels apply). The ACS questionnaire is extremely like that of the US Census Questionnaire, given that it’s meant to be a substitution for it. It was developed after the Census Bureau was provided with various subjects that other federal agencies justified as important, categorizing each subject as “mandatory”, “required”, or “programmatic”, from which the ACS collected data for both the “mandatory” and “required” subjects. This approach illustrates itself in the questionnaire, as every question is answered by an objective fact, whether it’s a checkbox, a number, or a short answer (such as the person’s major). This allows little ambiguity in how the question must be answered, and makes each response fairly easy, which is a major strength of the questionnaire. However, depending on the number of people in the residence, the survey can be on the longer side (up to 11 pages). Combined with the fact that it is legally mandated that the response is filled out, with a potential fine of up to $5000, respondents may feel the need to rush through or give a fake response to questions they may not know immediately (such as when the residence was built). In addition, in the case that there is not one single person responsible for the residence, like in the case where multiple people are renting a room from one landlord who doesn’t live at the residence, then the data of those who are not “Person 1” is not collected, despite the fact that they are on the same standing in the eyes of the survey. However, no one has been prosecuted for not filling out the survey since 1970 and the case outlined in the second point is relatively rare in the total population, so these weaknesses are fairly minor in the grand scheme of things. 

As mentioned above, the variables we used were age (the age of the respondent), state (the state they lived in), gender (what gender they identified as), education (what their highest level of education was), and race/ethnicity of the respondent. However, as the post-stratification data was a different dataset to that of the survey data, we had to transform the post-stratification data to match that, including general cleaning in the sense of converting non-numeric values to numeric, matching up spelling/capitalization of certain responses, and constructing an age_groups variable from the ages given. For more information, please see 01-data_cleaning-post-strat.R in the scripts folder of the github repository.   <-- This section is to be updated accordingly

```{r post-strat graphs}
# Plot Race
perc_race <- cleaned_data %>% count(race_ethnicity) %>% mutate(perc = n/nrow(cleaned_data))
race <- perc_race %>% ggplot(aes(x = race_ethnicity, y = perc)) + geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + labs(title = "Race/Ethnicity of Respondents in 2018 ACS data",
                                                                               x = "Race/Ethnicity", y = "Percentage", subtitle = "Figure X")
race

# Plot Gender
perc_gender <- cleaned_data %>% count(gender) %>% mutate(perc = n/nrow(cleaned_data))
gender <- perc_gender %>% ggplot(aes(x = gender, y = perc)) + geom_bar(stat = "identity") + 
  labs(title = "Gender of Respondents in 2018 ACS data", x = "Gender", y = "Percentage", subtitle = "Figure X")
gender

# Plot education
perc_education <- cleaned_data %>% count(education) %>% mutate(perc = n/nrow(cleaned_data))
perc_education$education <- perc_education$education %>% factor(levels = c("High School or less", "Post Secondary Degree"))
education <- perc_education %>% ggplot(aes(x = education, y = perc)) + geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + labs(title = "Education of Respondents in 2018 ACS data",
                                                                               x = "Education", y = "Percentage", subtitle = "Figure X")
summary(cleaned_data$education)
education

# Explain 88 Olds are shit


# Plot state
perc_state <- cleaned_data %>% count(state) %>% mutate(perc = n/nrow(cleaned_data))
perc_state$state <- perc_state$state %>% factor(levels = sort(as.character.factor(perc_state$state))) 
state <- perc_state %>% ggplot(aes(x = state, y = perc)) + geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + labs(title = "State of Respondents in 2018 ACS data",
                                                                               x = "State", y = "Percentage", subtitle = "Figure X")
state

# Plot age_group
perc_age_group <- cleaned_data %>% count(age_group) %>% mutate(perc = n/nrow(cleaned_data))
#perc_age_group$age_group <- perc_age_group$age_group %>% factor(levels = sort(as.character.factor(perc_age_group$age_group))) 
age_group <- perc_age_group %>% ggplot(aes(x = age_group, y = perc)) + geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + labs(title = "Age of Respondents in 2018 ACS data",
                                                                               x = "Age Group", y = "Percentage", subtitle = "Figure X")
age_group
```


# Model

\begin{equation}
Pr(\theta | y) = \frac{Pr(y | \theta) Pr(\theta)}{Pr(y)}  (\#eq:bayes)
\end{equation}

Equation \@ref(eq:bayes) seems useful, eh?

Our model is built on MRP, which stands for multi-level regression with post-stratifications. We utlize multi-level regression in order to create a model that we can build predictions upon. Since our predictor y is a binary variable (Biden or Trump), we used a logistic regression in order build our model. A logistic regression predicts the likelihood of an outcome based on various independent variables and it fits a model on a log curve which is bound in the Y axis in the range of [0,1].. In our case, we judge the likelihood that an individual would vote for Joe Biden in comparison to Donald Trump, given the individuals age, gender, education, state, and race. Given these inputs and outputs, we can train our logistic regression model using BRMS to predict which values for each variable predicts certain votes, and this will be discussed further in the later section. In our model, as it is a binary predictor, we round the prediction to the nearest number (more than 0.5 is rounded to 1, meaning this individual is likely to vote for Joe Biden, and less than 0.5 is rounded to 0, meaning this individual is likely to vote to Donald Trump).

One variable we discussed about including was annual income, however when we added the income lowered our cross validation score and the well discussed issues with self-reported income (bias, inaccurate estimations, not reporting) convinced us to disclude income. Another variable we debated on was education. Originally, education was included, however, education severely impacted our cross validation accuracy. We solved these issues by summarizing all education levels into 2 levels: at least some Post-Secondary and No post-secondary. This allowed our model to perform slightly better, but more importantly, there was a significant difference in the voting preference between the 2 groups. Another variable that helped increase accuracy after manipulation was age, where it became a better independent variable after breaking age into groups on 10's (eg. 18-28, 29-38) in order to better represent different age groups. Other groupings we tried was breaking into youth, middle age, and seniors (18-35, 36-55, 55+) and leaving age as individual numeric values, however both of these breakdowns did not provide any significant value. 

Overall, our model performs fairly (Figure N), with a cross-validation accuracy of approximately 63% (Figure N). This performed better than 50% (guessing) and through this we are able to gain some insight on how each variable affect the prediction. Furthermore, by investigating our inaccuracies (Figure N), we see that of the approximately 21% of our predictions were false negatives (Predicted Trump vote when it should predict Biden) and 15% of our predictions were false positives (predicted Biden when it should have predicted Trump vote). This was significantly better than some of the other models we tried as discussed in the previous paragraphs, as we had a large number of false negatives in many models when education was split into multiple groups rather than just College or no College. 

```{r}
model_education <- readRDS("../../model/4chains_3000iter_.rds")

reduced_data <- readRDS("../../inputs/training_data.Rda")


total_rows <- nrow(reduced_data)

# get updated columns (used for the model)
reduced_data <- reduced_data %>% select(vote_biden,age_group, education, gender,
                                        race_ethnicity, state)

set.seed(50)
#set
shuffle_indices <- sample(total_rows)
data <- reduced_data[shuffle_indices,]
boundary <- as.integer(total_rows*0.95)

training <- data[0:boundary,]

testing <- data[boundary:total_rows,]

#testing the accuracy of the model
probability <- predict(model_education, type = "response", newdata = testing)
probability <- if_else(probability[,1] >0.5,1,0)
testing$probs <- probability
testing <- testing %>% mutate(acc = probs==vote_biden)
data.frame(table(testing$acc)) %>% 
  ggplot(aes(x = Var1, y = Freq/(sum(Freq)))) +
  geom_bar(stat="identity") +
  xlab("Correct Prediction") +
  ylab("Percent") +
  ggtitle("Cross Validation Scores")
```


```{r}
wrong <- as.numeric(testing$vote_biden) - as.numeric(testing$probs) - 1


wrong <- plyr::mapvalues(wrong, c(-1,0,1), c("False Positive", 
                                                   "Correct",
                                                   "False Negative"))

data.frame(table(wrong)) %>% 
  ggplot(aes(x = wrong, y = Freq/sum(Freq)))+
  geom_bar(stat="identity") +
  xlab("Prediction Correctness") +
  ylab("Total")+
  ggtitle("Cross Validation inaccuracies")

```
We cannot directly apply our model on the ACS dataset as the dataset may not represent the American demographics accurately. For example, females tends to respond more to surveys than males, therefore in our dataset we may have 60% females and 40%males, meaning that our vote count with be biased towards females (eg. if females tends to vote Biden, we would have a bias towards Biden). Realistically, males and females should be around 50/50, so post-stratification allows us to adjust these weighting accordingly.
When we perform post-stratification on ACS, we find all combinations of our variables and find the weight of each combinations representation within USA. Using PERWT variable provided by the ACS, we can calculate how much of the population each combination represents within the United States. As per the UPUMS webpage, "PERWT indicates how many persons in the U.S. population are represented by a given person in an IPUMS sample", meaning we can add all the people within the same combination of variables we are measuring and add their PERWT in order to have an estimate of how much the combination weighs. 

Using MRP provides many benefits. As mentioned, we can more accurately estimate the weight of our sample predictions in relation to the population, therefore we will not be heavily affected by the any bias the survey may have. If the survey does not have any bias, our estimates will not dramatically alter any proportions. It also allows us to use a small sample, and apply it to a much larger sample that better represents the population. In our example, we can use a  small scale election survey from Nationscape and train a model which allows us to predict the election results from the American Community Survey consisting of over 3 million data. Since the ACS dataset does not contain the main information we want (voting), we use Nationscape's survey to predict results. This is crucial, as collecting surveys with a useful sample size with regards to an upcoming election can be extremely expensive and time consuming, as these poll results may be time sensitive. The option to collect from a small sample size allows staticians to save money and time meanwhile providing significant results about the larger population by combining their surveys to general census information. 

However, there are also cons to using MRP. For instance, we are limited to only use variables that are common and can be mapped to contain the same Fields in order to work. If one dataset has religion, and the second dataset does not, we are unable to include religion in our model. This can be challenging as information such as an individuals vote in 2016 may be extremely important as a variable, but since the ACS dataset does not contain such information, we are use it to our advantage. Further more, if a variable is broken down to a different granularity, we must group by the common group. For example, if dataset 1 has age grouped in age ranges of 3 and dataset 2 has age grouped in age ranges of 5 (eg. 5-8,9-12 vs 5-10,15-20) then we must map them into ranges that are multiples of 15. We cannot use groups of 3, because we cannot accurately break down groups of 5 into groups of 3. Therefore we lose granularity in situations where it may be desired. Another noted benefit is efficiency when predicting. Instead of inidivually predicting over 3 million people, we summarize all the comparisons and only predict on over 8000 individuals and multiply their proportions, which allows us to compute our predictions significantly faster. 

In this case, using MRP makes sense as we have voting information provided by Nationscape, however this dataset does not contain a large enough sample size to estimate the population. We also have access to the ACS which contains enough data to estimate the population but it does not contain any information on who an individual may vote for in the upcoming election. Furthermore, there exists biases within our datasets which would skew results otherwise. Using MRP allows us to use both datasets in order to predict the overall U.s 2020 election

https://arxiv.org/pdf/1908.06716.pdf
https://www.sciencedirect.com/science/article/pii/S0261379409001176?casa_token=ec6M6gmXHpMAAAAA:GdXW-ErZjnoZ4GYv_TEPydUAhwppRK0GbPth6kXbSebh8Iz2kmmIMCc1LgczlhByHSMr57NGGmsp

Here's a dumb example of how to use some references: In paper we run our analysis in `R` [@citeR]. We also use the `tidyverse` which was written by @thereferencecanbewhatever If we were interested in baseball data then @citeLahman could be useful. 
@tausanovitch_vavreck_2019

# Results
Our data is of penguins (Figure \@ref(fig:bills)).

```{r bills, fig.cap="Bills of penguins", echo = FALSE}
ggplot(penguins, aes(x = island, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange","purple","cyan4"),
                    guide = FALSE) +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()
```

Talk more about it.

Also bills and their average (Figure \@ref(fig:billssssss)). (Notice how you can change the height and width so they don't take the whole page?)

```{r billssssss, fig.cap="More bills of penguins", echo = FALSE, fig.width=8, fig.height=4}

# This needs to be about the random data tha tI sasmpled?
ggplot(penguins, aes(x = island, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange","purple","cyan4"),
                    guide = FALSE) +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()
```

Talk way more about it. 
# Discussion

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

# Appendix {-}

\newpage


# References
R
Survey and ACS dataset
https://www.voterstudygroup.org/uploads/reports/Data/Nationscape-User-Guide_2020sep10.pdf
https://www.voterstudygroup.org/uploads/reports/Data/NS-Methodology-Representativeness-Assessment.pdf

Tausanovitch, Chris and Lynn Vavreck. 2020. Democracy Fund + UCLA Nationscape, October 10-17, 2019 (version 20200814). Retrieved from [URL].

