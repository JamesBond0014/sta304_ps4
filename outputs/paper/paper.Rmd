---
title: "Biden Expected to Win 54% of Popular Vote in 2020 US Presidential Election"
subtitle: "4% Confidence Interval Based on a Survey from June 2020"
author: "James Bao, Zakir Chaudry, Alan Chen, Xinyi Zhang"
date: "`r format(Sys.time(), '%d %B %Y')`"
thanks: "Code and data supporting this analysis are available at: https://github.com/JamesBond0014/sta304_ps4."
abstract: |
  | After a shocking upset in the 2016 US Presidential election, everyone has their eyes on the 2020 election to determine the leader of the free world for the next four years. In this Paper we trained a model using survey level survey results . Our model predicts that Joe Biden will win the 2020 Presidential election with a 53.5% of the popular vote and a confidence level of 95%. Our prediction as well as our breakdown of votes by demographic group could potentially provide the candidates of the election with information on how to target voters.
  |
  | **Keywords:** forecasting, US 2020 Election, Trump, Biden, multilevel regression with post-stratification

output:
  bookdown::pdf_document2:
toc: FALSE
bibliography: references.bib
---

```{r setup, include=FALSE}
library(haven)
library(tidyverse)
library(knitr)
library(palmerpenguins)
```
```{r loading}
# Load required data and models
cleaned_data <- data.frame(read.csv(file = "../../inputs/cleaned_acs.csv"))
```

# Introduction

# Data
To train our model to predict the outcome of the 2020 US presidential election, we used Wave 49 of the Nationscape Dataset (results from the week of June 18-24, 2020). We will discuss how this data was collected, its key features, and what the data looks like in the section titled "Individual-level survey dataset."

TODO: To make predictions on the outcome of the 2020 US presidential election, we used... We will discuss how this data was collected, its key features, and what the data looks like in the section titled "Post-stratification dataset." The explanation of multilevel modelling with post-stratification can be found in the "Model" section.

## Individual-level survey dataset

### Data collection
The Nationscape Project is 16-month-long voter study (from July 2019 to January 2021) that conducts weekly surveys regarding the 2020 US presidential election. We will mainly discuss Wave 49 of the Nationscape Dataset for the reminder of this paper.

From June 18, 2020 to June 24, 2020, Nationscape collected data on public opinion about the 2020 presidential campaign and election by conducting 15-minute online interviews. Their target is the American "population." Unfortunately, the published information on their methodology is not more specific as to what constitutes a member of the American population. Presumably (based on analyzing the data), their target population is all adult individuals presently residing in the United States. 

Nationscape used the audience of market research platform Lucid as its sampling frame. Sampling frames are lists of the units (individuals in our case) that will be selected for the survey sample, meaning that the survey respondents on Lucid form a list of a subset of the target population (from which a sample will be taken). Finally, a sample matching the demographics of the American population is selected from the frame using a purposive sampling method. This is a non-probability sampling method where the researcher decides which samples are most representative of the target population. More specific information about their sampling method was not provided (besides a statement that the sampling was not random). After being contacted by Lucid to take the survey, respondents are immediately redirected to Nationscape survey software where the questionnaire starts. 

Nationscape reported that the nonresponse rate was about 17%. Another 8% of responses were removed for speeding (spending less than 6 minutes completing the survey) or for "straight-lining" answers (selecting the same response for all policy questions) resulting in a final sample size of 6,532 respondents. To reduce the effects of non-response bias and to ensure results were representative of the US population, survey responses were weighted using data from the 2018 American Community Survey (for demographic variables) and from the United States Elections Project and MIT Election Lab (for 2016 vote). This ensures that the discrepancy between the target population and survey responses is minimized. Lastly, Nationscape assessed the representativeness of the survey sample by including questions from the 2018 Pew evaluation of non-probability samples and comparing their results to Pew findings and government benchmarks. Overall, the difference between Nationscape results and government benchmarks was comparable to the difference between Pew findings and government benchmarks. Consequently, Nationscape concluded that estimates from their dataset should be considered sufficiently valid (at least in comparison to other political polling non-probability samples analyzed by Pew).

The strengths of Nationscape's survey methodology include pilot testing their questionnaire for several weeks, which allowed staff to finetune survey questions and respondent selection criteria. Along the same line, the survey strikes a good balance between being detailed enough to capture useful data while being short enough to hold respondent attention. Furthermore, the high frequency of the data collection process provides the dataset with a week by week breakdown of voter sentiment, potentially capturing changes in public political opinion as news or controversies break. Lastly, the response rate is extremely good for an online survey, indicating that the vast majority of the selected sample responded. In fact, a response rate of over 80% is very high and likely due to the distribution of the survey through the Lucid platform (and certain characteristics of or certain incentives for survey respondents on the platform). 

On the other hand, a major weakness of the survey is that sampling was not conducted at random but rather demographic criterias were designed by the Nationscape staff. Another weakness is that the sampling frame is not necessarily representative of the American population (those who aren't members of survey panels or aren't comfortable sharing political opinions are likely not represented). Lastly, the results are likely subjected to response bias because of the subjective nature of the research topic. However, as previously mentioned, Nationscape addressed these weaknesses by comparing their results to the results from 2018 Pew evaluations on non-probability sampling (and found the accuracy and representativeness of their dataset to be comparable). 

### Data features and visualization
The full dataset for Wave 49 consists of 6,532 responses for over 260 variables. They cover topics ranging from the presidential candidates to government policies, current events, political views and respondent demographics. In the interest of brevity, we will focus our discussion on the explanatory and response variables relevant to our model. We aim to predict the winner of the popular vote in the 2020 US presidential election so our response variable of choice is vote_2020. We chose age, gender, race_ethnicity, state, and education as explanatory variables based on the demographic characteristics that are most important in determining user vote and our ability to match these variables with the post stratification dataset. In greater detail, here are the chosen variables:

- vote_2020: the vote of the respondent given that the Democratic nominee is Joe Biden and the Republican nominee is Donald Trump 

Table 1: Respondent 2020 US presidential election vote distribution
```{r, echo=FALSE}
raw_data <- read_dta("../../inputs/data/ns20200625/ns20200625.dta")
raw_data <- labelled::to_factor(raw_data)

vote_2020 = table(raw_data$vote_2020)
kable(vote_2020, col.names = c("vote_2020", "Frequency"))
```
- age: the age of the respondent in years at the time of the survey

Table 2: Respondent age statistics
```{r, echo=FALSE}
age = as.array(summary(raw_data$age))
kable(age, col.names = c("Statistics", "Values"))
```

- gender: the sex of the respondent (the options being "Male" or "Female")

Table 3: Respondent gender distribution
```{r, echo=FALSE}
gender = table(raw_data$gender)
kable(gender, col.names = c("gender", "Frequency"))
```

- race_ethnicity: the race of the respondent 

Table 4: Respondent race distribution
```{r, echo=FALSE}
race_ethnicity = table(raw_data$race_ethnicity)
kable(race_ethnicity, col.names = c("race_ethnicity", "Frequency"))
```

- state: the state the respondent resides in (table omitted in the interest of space)

- education: the highest level of education completed by the respondent

Table 5: Respondent education distribution
```{r, echo=FALSE}
education = table(raw_data$education)
kable(education, col.names = c("education", "Frequency"))
```

For the variables age, gender, race_ethnicity, state, and education, we did not find similar equivalents in the dataset. We did find that the variable trump_biden (the candidate that the respondent would support if the election was a contest between Donald Trump and Joe Biden) was similar to our selected variable vote_2020. However, as vote_2020 is more representative of the nature of the popular vote, we did not end up choosing trump_biden. 

When cleaning the data, we merged some of the factors of the variables to match the granularity of the data in the post-stratification dataset. This included splitting age responses into bins of size 10, reducing education to two bins ("High School or Less" and "Post Secondary or More"), and combining Asian Indian, Korean, Filipino, Vietnamese, and Pacific Islander ethnicities into "Asian (Other)" (Chinese and Japanese remained their own factors because this is the level of specificity available in the post-stratification dataset). Lastly, we took a subset of the dataset where respondents had decided to vote for either Trump or Biden in the 2020 US presidential election (for the purposes of being able to predict the popular vote using a binary model).

```{r, echo=FALSE}
reduced_data <- 
  raw_data %>% 
  select(vote_2020,
         age,
         gender,
         race_ethnicity,
         state,
         education)

reduced_data <- reduced_data[complete.cases(reduced_data),]

# Remove people under 18 and over 78 
reduced_data <- reduced_data[reduced_data$age <= 78 &
                               reduced_data$age >= 18,]

# Splitting age responses into groups of 10 years
reduced_data$age_group <- cut(reduced_data$age, 
                              breaks = seq(10, 88, 10),
                              labels = c("18 to 28","29 to 38","39 to 48",
                                         "49 to 58","59 to 68","69 to 78", 
                                         "79 and above"), 
                              right=FALSE)


# Combine education options to match with post stratification data
reduced_data$education <- as.character(reduced_data$education)
reduced_data$education[
  grepl("Completed some graduate, but no degree", reduced_data$education)|
  grepl("Masters degree", reduced_data$education)|
  grepl("Doctorate degree", reduced_data$education)|
  grepl("Associate Degree", reduced_data$education)|
  grepl("college", tolower(reduced_data$education))
] <- ('Post Secondary Degree')
reduced_data$education[
  grepl("Grade", reduced_data$education)|
  grepl("high school", tolower(reduced_data$education))
] <- ('High School or Less')
reduced_data$education <- as.factor(reduced_data$education)

# Combine race factors to match with post stratification data
reduced_data$race_ethnicity <- as.character(reduced_data$race_ethnicity)
reduced_data$race_ethnicity[
  reduced_data$race_ethnicity == 'Asian (Asian Indian)' |
    reduced_data$race_ethnicity == 'Asian (Korean)' |
    reduced_data$race_ethnicity == 'Asian (Filipino)' |
    reduced_data$race_ethnicity == 'Asian (Vietnamese)'|
    grepl("Pacific", reduced_data$race_ethnicity)
] <- ('Asian (Other)')
reduced_data$race_ethnicity <- as.factor(reduced_data$race_ethnicity)

# Filtering for only Trump and Biden voters
reduced_data <- reduced_data[reduced_data$vote_2020 == "Donald Trump" |
                             reduced_data$vote_2020 == "Joe Biden",]
# Change vote to binary variable: 1 for voting for Biden, 0 for voting for Trump
reduced_data$vote_biden <- plyr::mapvalues(reduced_data$vote_2020, 
                                           c("Donald Trump", "Joe Biden"),
                                           c(0, 1))
# Drop unused levels
reduced_data <- droplevels(reduced_data)
```

The distributions of each of our cleaned variables (with the exception of gender and state) are shown in the following two pages. 

```{r agedist, fig.height=4, fig.cap="Distribution of the age of respondents in percentages.", message=FALSE, echo=FALSE}
# total number of observations in the dataset
total_count = nrow(reduced_data)

# summary table with counts for sex
tibble_age <- data.frame(table(reduced_data$age_group)) %>%
  rename(
    age_group = Var1,
    count = Freq
  )
# distribution of sex as a bar graph in percentages
ggplot(tibble_age, aes(x = age_group, y = count/total_count)) + 
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(count/total_count*100), "%")),
            color="white",
            position = position_stack(vjust = 0.5)) +
  xlab("Age") +
  ylab("Percentage of Respondents")
```


```{r racedist, fig.height=4, fig.cap="Distribution of the race of respondents in percentages.", message=FALSE, echo=FALSE}
# summary table with counts for race
tibble_race <- data.frame(table(reduced_data$race_ethnicity)) %>%
  rename(
    race = Var1,
    count = Freq
  )
tibble_race$race <- factor(tibble_race$race, 
                                      levels = c("White", "Black, or African American", 
                                                 "Asian (Chinese)", "Asian (Japanese)",
                                                 "Asian (Other)", "American Indian or Alaska Native", "Some other race"))
# distribution of sex as a bar graph in percentages
ggplot(tibble_race, aes(x = race, y = count/total_count)) + 
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(count/total_count*100), "%")),
            color="black",
            position = position_stack(vjust = 1)) +
  theme(axis.text.x = element_text(angle = 45, hjust=1)) +
  xlab("Race") +
  ylab("Percentage of Respondents")
```

```{r educationdist, fig.height=4, fig.cap="Distribution of the education of respondents in percentages.", message=FALSE, echo=FALSE}
# summary table with counts for race
tibble_education <- data.frame(table(reduced_data$education)) %>%
  rename(
    education = Var1,
    count = Freq
  )
# distribution of sex as a bar graph in percentages
ggplot(tibble_education, aes(x = education, y = count/total_count)) + 
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(count/total_count*100), "%")),
            color="white",
            position = position_stack(vjust = 0.5)) +
  xlab("Education") +
  ylab("Percentage of Respondents")
```

```{r votedist, fig.height=4, fig.cap="Distribution of the vote of respondents in percentages.", message=FALSE, echo=FALSE}
# summary table with counts for race
tibble_vote <- data.frame(table(reduced_data$vote_2020)) %>%
  rename(
    vote = Var1,
    count = Freq
  )
# distribution of sex as a bar graph in percentages
ggplot(tibble_vote, aes(x = vote, y = count/total_count)) + 
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(count/total_count*100), "%")),
            color="white",
            position = position_stack(vjust = 0.5)) +
  xlab("Vote") +
  ylab("Percentage of Respondents")
```

Out of the respondents, approximately 50% were female and 50% were male. Most of the respondents were in their 60's and 70's, while the next most common demographic were respondents in their 40's and 50's (Figure 1). This preliminary look at the dataset is fairly consistent with Canadian demographics according to the 2016 Census, with the female response being approximately 3% higher than expected and the average age being approximately 11 years older than expected (the average Canadian age is 41 while the average respondent age was 52). This older demographic makes sense as individuals less than 15 years of age were not eligible to respond to the survey and are therefore not represented here. 

Of the 20,602 responses, 194 rows were dropped if the value for a variable was not available. For our purposes of attempting to model mental health, we consequently removed these individuals from the dataset we used in generating our model. Furthermore, according to Figure 5, the responses are heavily skewed towards positive responses, with 30% of respondents replying with 'Excellent' and 34% replying 'Very good'. 28% rated their mental health as 'Good' with the remaining 8% split 6 to 2 with regards to 'Fair' and 'Poor', respectively. These results overwhelmingly indicate that a large proportion of the sampled population feel that their mental is very strong. However, we proceed with modeling in the next section of this paper to better understand the contribution of the chosen demographic and family factors on self-rated mental. Is there a pattern of traits that separate "Excellent", "Very good", and "Good" ratings? What are the biggest distinctions between an individual with good mental health and poor mental health? These are some of the motivating questions we strive to answer with our model.

## Post-stratification dataset
The post-stratification dataset was gathered from the American Community Survey (ACS), a project aiming to mitigate issues stemming from the census’ 10-year intervals by providing an annualized version of data like that produced by the decennial census long form. The dataset used in this study is specifically that of the 2018 ACS data. The ACS data can be accessed at the IPUMS website. More details on attaining and cleaning data are found in 01-data_cleaning-post-strat.R in the scripts folder of the git repository.

The target population, much like the census, is essentially anyone who resides in a dwelling in the US. Following this, the sampling frame of the ACS is the Master Address File that is maintained by the US Census Bureau. Created for the 2000 Census, it was originally based on the 1990 Address Control File and the United States Postal Service’s Delivery Sequence File. The maintaining and updating of this file is crucial to the efforts of the ACS and any other body that makes use of it. In addition, the ACS samples 2.5 percent of the population living “Group Quarters”, non-housing units (eg. nursing homes, prisons, college dorms, etc.). In total, the 2018 ACS data contains about 3.2 million observations, sampled from across the country.

Every month, a systemic sample is created for each US county or equivalent, where they are mailed the ACS survey at the start of the month. As of February 2002, the sampling rate of all counties has been 2.5%, except for Houston, Texas, which is sampled at 1% (due to the size of the population). For every site, the sampling is broken into two steps. The first step is sampling 17.5% of the population, which is then subsampled from to achieve that desired percentage. All non-respondents are subsequently contacted by phone for a computer assisted telephone interview one month later. One third of non-respondents that have reached this point are then sampled from to be contacted for a computer assisted personal interview following the previous telephone interview attempt. Beyond this sample (referred to as the National Sample or Supplemental Sample), data was also collected at 31 selected test sites to represent areas with various county population sizes or areas that were difficult to enumerate. The ACS data is weighted in order to ensure reliable and usable estimated regarding the population. 

The ACS Questionnaire asks questions regarding every inhabitant in the residence it is sent to. However, the most information is required of “Person 1”, the person whose name the residence is owned in, being bought in, or rented in (or any adult, if none of those labels apply). The ACS questionnaire is extremely like that of the US Census Questionnaire, given that it’s meant to be a substitution for it. It was developed after the Census Bureau was provided with various subjects that other federal agencies justified as important, categorizing each subject as “mandatory”, “required”, or “programmatic”, from which the ACS collected data for both the “mandatory” and “required” subjects. This approach illustrates itself in the questionnaire, as every question is answered by an objective fact, whether it’s a checkbox, a number, or a short answer (such as the person’s major). This allows little ambiguity in how the question must be answered, and makes each response fairly easy, which is a major strength of the questionnaire. However, depending on the number of people in the residence, the survey can be on the longer side (up to 11 pages). Combined with the fact that it is legally mandated that the response is filled out, with a potential fine of up to $5000, respondents may feel the need to rush through or give a fake response to questions they may not know immediately (such as when the residence was built). In addition, in the case that there is not one single person responsible for the residence, like in the case where multiple people are renting a room from one landlord who doesn’t live at the residence, then the data of those who are not “Person 1” is not collected, despite the fact that they are on the same standing in the eyes of the survey. However, no one has been prosecuted for not filling out the survey since 1970 and the case outlined in the second point is relatively rare in the total population, so these weaknesses are fairly minor in the grand scheme of things. 

As mentioned above, the variables we used were age (the age of the respondent), state (the state they lived in), gender (what gender they identified as), education (what their highest level of education was), and race/ethnicity of the respondent. However, as the post-stratification data was a different dataset to that of the survey data, we had to transform the post-stratification data to match that, including general cleaning in the sense of converting non-numeric values to numeric, matching up spelling/capitalization of certain responses, and constructing an age_groups variable from the ages given. For more information, please see 01-data_cleaning-post-strat.R in the scripts folder of the github repository.   <-- This section is to be updated accordingly

```{r create post-strat graphs, message=FALSE, echo=FALSE}
cleaned_data <- labelled::to_factor(cleaned_data)

# Plot Race
perc_race <- cleaned_data %>% count(race_ethnicity) %>% mutate(perc = n/nrow(cleaned_data))
race <- perc_race %>% ggplot(aes(x = race_ethnicity, y = perc)) + geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + labs(title = "Race/Ethnicity of Respondents in 2018 ACS data", x = "Race/Ethnicity", y = "Percentage", subtitle = "Figure X")

# Plot Gender
perc_gender <- cleaned_data %>% count(gender) %>% mutate(perc = n/nrow(cleaned_data))
gender <- perc_gender %>% ggplot(aes(x = gender, y = perc)) + geom_bar(stat = "identity") + 
  labs(title = "Gender of Respondents in 2018 ACS data", x = "Gender", y = "Percentage", subtitle = "Figure X")

# Plot education
perc_education <- cleaned_data %>% count(education) %>% mutate(perc = n/nrow(cleaned_data))
perc_education$education <- perc_education$education %>% factor(levels = c("High School or less", "Post Secondary Degree"))
education <- perc_education %>% ggplot(aes(x = education, y = perc)) + geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + labs(title = "Education of Respondents in 2018 ACS data",
                                                                               x = "Education", y = "Percentage", subtitle = "Figure X")

# Explain 88 Olds are shit

# Plot state
perc_state <- cleaned_data %>% count(state) %>% mutate(perc = n/nrow(cleaned_data))
perc_state <- labelled::to_factor(perc_state)

state <- perc_state %>% ggplot(aes(x = state, y = perc)) + geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + labs(title = "State of Respondents in 2018 ACS data", x = "State", y = "Percentage", subtitle = "Figure X")

# Plot age_group
perc_age_group <- cleaned_data %>% count(age_group) %>% mutate(perc = n/nrow(cleaned_data))
age_group <- perc_age_group %>% ggplot(aes(x = age_group, y = perc)) + geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + labs(title = "Age of Respondents in 2018 ACS data",
                                                                               x = "Age Group", y = "Percentage", subtitle = "Figure X")
```
```{r post-strat graphs,fig.height=6, message=FALSE, echo=FALSE}
race
education
gender
state
age_group
```

# Model

The purpose of our model is to predict a person's vote in the 2020 US election given their characteristics (eg. income, race, gender, etc.). The dependent variable is a binary categorical variable, where 0 represents voting for Trump and 1 represents voting for Biden. The independent variables are the characteristics of the person. To avoid multicollinearity which causes unreliable regression estimates, a few carefully selected independent variables were used for our model. One criteria for the selection was that the variables had to be both present in the Nationscapes dataset and the ACS dataset, so that they be included for post-stratification. Another criteria is that the characteristic should group people that share a similar perspective which may impact on their voting decision. Through literature research (Uppal and LaRochelle-Cote, 2015), we found that some helpful indicators were age, gender, education, state, race, and self-reported income. To finalize the selection, characteristics should not be highly correlated and the data provided should make logical sense. Since the data quality for income was generally poor and it worsened the model performance, income was included in the model. Therefore, the predictors used for the model are age, gender, education, state, and race.

Since the goal is to do binary classification using multiple explanatory variables, binary logistic regression is a suitable method for the model. We used logistic regression to predict the probability of a person's voting choice and determine their vote by rounding to the nearest represented binary categorical variable. Logistic regression takes independent variables as inputs and based on the assigned weights and a logit function, the output will be a probability [0,1] of the outcome happening. Below is the equation of the logistic regression model:

\begin{equation}
Pr(Y_i \in \{Trump, Biden\}) = logit^{-1}(\alpha_{j[i]}^{\textrm{age}} + \alpha_{j[i]}^{\textrm{gender}} + \alpha_{j[i]}^{\textrm{edu}} + \alpha_{j[i]}^{\textrm{state}} + \alpha_{j[i]}^{\textrm{race}})
\end{equation}

The Nationsacpes dataset contains the characteristics of a voter and also their vote, so the model was trained by using cross validation split on that dataset. 95% of the dataset was used as the training set to determine the best weights for the model and the remaining 5% of the dataset was used for the test set to verify the accuracy of the model. The vast majority of the data is used for the training set because we wanted to ensure that the data does not overfit due to lack of data. The logistic regression model was implemented using the BRMS library.

Furthermore, there were many adjustments that had to be done with the inputs to improve model accuracy. Apart from self-reported income, education was another independent variable that was debated on whether it should be included in the model. Education severely impacted our cross validation accuracy, due to a wide range of values which were too specific for our purpose. These issues were resolved by summarizing all education levels into 2 levels: At least some Post-Secondary and No post-secondary. This allowed our model to perform slightly better, but more importantly, there was a significant difference in the voting preference between the 2 groups. 

Another variable that initially caused inaccurate results was age. There was no clear trend in having age as a numeric continuous value for predicting votes. As a result, the numerical age values were grouped into 10's (eg. 18-28, 29-38) in order to better represent different age groups. Other groupings we tried was breaking into youth, middle age, and seniors (18-35, 36-55, 55+); however, that split was not able to capture the pattern in the age groups because the model accuracy worsened. So grouping age by 10's was the adjustment that gave better results and this is the configuration that was included in the model.

Overall, our model performs fairly (Figure N), with a cross-validation accuracy of approximately 63% (Figure N). This performed better than 50% (guessing) and through this we are able to gain some insight on how each variable affect the prediction. Furthermore, by investigating our inaccuracies (Figure N), we see that of the approximately 21% of our predictions were false negatives (Predicted Trump vote when it should predict Biden) and 15% of our predictions were false positives (predicted Biden when it should have predicted Trump vote). This was significantly better than some of the other models we tried as discussed in the previously, as we had a large number of false negatives in many models when education was split into multiple groups rather than just College or no College.  

```{r}
model_education <- readRDS("../../model/4chains_3000iter_.rds")

reduced_data <- readRDS("../../inputs/training_data.Rda")


total_rows <- nrow(reduced_data)

# get updated columns (used for the model)
reduced_data <- reduced_data %>% select(vote_biden,age_group, education, gender,
                                        race_ethnicity, state)

set.seed(50)
#set
shuffle_indices <- sample(total_rows)
data <- reduced_data[shuffle_indices,]
boundary <- as.integer(total_rows*0.95)

training <- data[0:boundary,]

testing <- data[boundary:total_rows,]

#testing the accuracy of the model
probability <- predict(model_education, type = "response", newdata = testing)
probability <- if_else(probability[,1] >0.5,1,0)
testing$probs <- probability
testing <- testing %>% mutate(acc = probs==vote_biden)
data.frame(table(testing$acc)) %>% 
  ggplot(aes(x = Var1, y = Freq/(sum(Freq)))) +
  geom_bar(stat="identity") +
  xlab("Correct Prediction") +
  ylab("Percent") +
  ggtitle("Cross Validation Scores")
```


```{r}
wrong <- as.numeric(testing$vote_biden) - as.numeric(testing$probs) - 1


wrong <- plyr::mapvalues(wrong, c(-1,0,1), c("False Positive", 
                                                   "Correct",
                                                   "False Negative"))

data.frame(table(wrong)) %>% 
  ggplot(aes(x = wrong, y = Freq/sum(Freq)))+
  geom_bar(stat="identity") +
  xlab("Prediction Correctness") +
  ylab("Total")+
  ggtitle("Cross Validation inaccuracies")

```
We cannot directly apply our model on the ACS dataset as the dataset may not represent the American demographics accurately. For example, females tends to respond more to surveys than males, therefore in our dataset we may have 60% females and 40%males, meaning that our vote count with be biased towards females (eg. if females tends to vote Biden, we would have a bias towards Biden). Realistically, males and females should be around 50/50, so post-stratification allows us to adjust these weighting accordingly.
When we perform post-stratification on ACS, we find all combinations of our variables and find the weight of each combinations representation within USA. Using PERWT variable provided by the ACS, we can calculate how much of the population each combination represents within the United States. As per the UPUMS webpage, "PERWT indicates how many persons in the U.S. population are represented by a given person in an IPUMS sample", meaning we can add all the people within the same combination of variables we are measuring and add their PERWT in order to have an estimate of how much the combination weighs. 

Using MRP provides many benefits. As mentioned, we can more accurately estimate the weight of our sample predictions in relation to the population, therefore we will not be heavily affected by the any bias the survey may have. If the survey does not have any bias, our estimates will not dramatically alter any proportions. It also allows us to use a small sample, and apply it to a much larger sample that better represents the population. In our example, we can use a  small scale election survey from Nationscape and train a model which allows us to predict the election results from the American Community Survey consisting of over 3 million data. Since the ACS dataset does not contain the main information we want (voting), we use Nationscape's survey to predict results. This is crucial, as collecting surveys with a useful sample size with regards to an upcoming election can be extremely expensive and time consuming, as these poll results may be time sensitive. The option to collect from a small sample size allows staticians to save money and time meanwhile providing significant results about the larger population by combining their surveys to general census information. 

However, there are also cons to using MRP. For instance, we are limited to only use variables that are common and can be mapped to contain the same Fields in order to work. If one dataset has religion, and the second dataset does not, we are unable to include religion in our model. This can be challenging as information such as an individuals vote in 2016 may be extremely important as a variable, but since the ACS dataset does not contain such information, we are use it to our advantage. Further more, if a variable is broken down to a different granularity, we must group by the common group. For example, if dataset 1 has age grouped in age ranges of 3 and dataset 2 has age grouped in age ranges of 5 (eg. 5-8,9-12 vs 5-10,15-20) then we must map them into ranges that are multiples of 15. We cannot use groups of 3, because we cannot accurately break down groups of 5 into groups of 3. Therefore we lose granularity in situations where it may be desired. Another noted benefit is efficiency when predicting. Instead of inidivually predicting over 3 million people, we summarize all the comparisons and only predict on over 8000 individuals and multiply their proportions, which allows us to compute our predictions significantly faster. 

In this case, using MRP makes sense as we have voting information provided by Nationscape, however this dataset does not contain a large enough sample size to estimate the population. We also have access to the ACS which contains enough data to estimate the population but it does not contain any information on who an individual may vote for in the upcoming election. However analyzing our ACS dataset, we did not notice much biases within our groups. Using MRP allows us to use both datasets in order to predict the overall U.s 2020 election

https://arxiv.org/pdf/1908.06716.pdf
https://www.sciencedirect.com/science/article/pii/S0261379409001176?casa_token=ec6M6gmXHpMAAAAA:GdXW-ErZjnoZ4GYv_TEPydUAhwppRK0GbPth6kXbSebh8Iz2kmmIMCc1LgczlhByHSMr57NGGmsp

Here's a dumb example of how to use some references: In paper we run our analysis in `R` [@citeR]. We also use the `tidyverse` which was written by @thereferencecanbewhatever If we were interested in baseball data then @citeLahman could be useful. 
@tausanovitch_vavreck_2019

# Results

```{r age_vs_vote, fig.height=4, echo=FALSE}
# Helper function for graphing the distribution of votes by another variable 
grouped_bar_chart <- function(data, xlab, ylab, title){
  ggplot(data, 
       aes(y=grouped, x=value, fill=Vote, label=Vote))+
  geom_bar(stat="identity",position="dodge", width=0.8) +
  scale_x_continuous(ylab) + 
  scale_y_discrete(xlab) + 
  ggtitle(title)
}

# Restore names for Biden and Trump votes
reduced_data$vote_2020 <- plyr::mapvalues(reduced_data$vote_biden,
                                           c(1, 0), 
                                           c("Joe Biden", "Donald Trump"))

# Graph distribution of votes for each age
# Data parsing into format I want for chart
grouped <- reduced_data %>%
  count(age_group, vote_2020) %>%
  rename(
    count=n
  )

# Total number of votes
total_vote <- aggregate(grouped$count, 
                        by=list(age_group=grouped$age_group), FUN=sum)

#process data into desired graphing format
grouped<-left_join(grouped, total_vote, by="age_group") %>%
  rename(total=x) %>%
  mutate(value=count/total) %>%
  rename(grouped=age_group, Vote=vote_2020)

grouped_bar_chart(grouped, "Age", "Percentage", 
                  "Figure 1: Distribution of votes by age")

```
```{r gender_vs_vote, fig.height=4, echo=FALSE}
# Graph distribution of votes for each gender

# Data parsing into format I want for chart
grouped <- reduced_data %>%
  count(gender, vote_2020) %>%
  rename(
    count=n
  )

# Total number of votes
total_vote <- aggregate(grouped$count, 
                        by=list(gender=grouped$gender), FUN=sum)

#process data into desired graphing format
grouped<-left_join(grouped, total_vote, by="gender") %>%
  rename(total=x) %>%
  mutate(value=count/total) %>%
  rename(grouped=gender, Vote=vote_2020)

grouped_bar_chart(grouped, "Gender", "Percentage", 
                  "Figure 2: Distribution of votes by gender")

```
```{r race_vs_vote, fig.height=4, echo=FALSE}
# Graph distribution of votes for each race

# Data parsing into format I want for chart
grouped <- reduced_data %>%
  count(race_ethnicity, vote_2020) %>%
  rename(
    count=n
  )

# Total number of votes
total_vote <- aggregate(grouped$count, 
                        by=list(race_ethnicity=grouped$race_ethnicity), FUN=sum)

#process data into desired graphing format
grouped<-left_join(grouped, total_vote, by="race_ethnicity") %>%
  rename(total=x) %>%
  mutate(value=count/total) %>%
  rename(grouped=race_ethnicity, Vote=vote_2020)

grouped_bar_chart(grouped, "Race", "Percentage", 
                  "Figure 3: Distribution of votes by education level")
```
```{r education_vs_vote, fig.height=4, echo=FALSE}
# Graph distribution of votes for each education level

# Data parsing into format I want for chart
grouped <- reduced_data %>%
  count(education, vote_2020) %>%
  rename(
    count=n
  )

# Total number of votes
total_vote <- aggregate(grouped$count, 
                        by=list(education=grouped$education), FUN=sum)

#process data into desired graphing format
grouped<-left_join(grouped, total_vote, by="education") %>%
  rename(total=x) %>%
  mutate(value=count/total) %>%
  rename(grouped=education, Vote=vote_2020)

grouped_bar_chart(grouped, "Education", "Percentage", 
                  "Figure 4: Distribution of votes by education level")

```



## Post-stratification

```{r}
cell_counts <- readRDS("../../inputs/cell_counts.RDS")
training_data <- readRDS("../../inputs/training_data.RDA")
model <- readRDS("../../model/4chains_3000iter_.rds")

race_res <- readRDS("../../processed_data/race_res.RDS")
gender_res <- readRDS("../../processed_data/gender_res.RDS")
state_res <- readRDS("../../processed_data/state_res.RDS")
age_res <- readRDS("../../processed_data/age_res.RDS")
education_res <- readRDS("../../processed_data/education_res.RDS")

```

The post-stratification data of the 2018 ACS data showed that most of the variables had very little biases since the MRP estimates were very close to the original 2018 ACS data. In Figure N shown below, we see that our raw data proportions was very close to the estimate proportions. Furthermore, this figure forecasts that almost 90% Black, or African American voters will vote for Joe Biden, 75% to 80% of Chinese and Japanese voters are predicted to vote for Biden, while approximately only 45% of white and American Indian or Alaska Native voters are projected to Vote for Joe Biden instead of Donald Trump. Overall, ethnic minorities are projected to vote for Biden over Trump.

```{r raceproportions, fig.height=4,fig.width=8, fig.cap="Proportion of Biden Votes by race", message=FALSE, echo=FALSE}
race_res %>% 
  ggplot(aes(y = mean, x = forcats::fct_inorder(race_ethnicity), 
             color = "MRP estimate")) + 
  geom_point() +
  ylab("Proportion voting for Biden") + 
  xlab("Race") + 
  geom_point(data = training_data %>% 
               group_by(race_ethnicity, vote_biden) %>%
               dplyr::summarize(n = n()) %>% 
               group_by(race_ethnicity) %>% 
               mutate(prop = n/sum(n)) %>% 
               filter(vote_biden==1), 
             aes(race_ethnicity, prop, color = "ACS raw data")) +
  scale_color_manual(name = "", values = c("MRP estimate" = "black", 
                                           "ACS raw data" = "red")) + 
  theme_bw(base_size = 14) + coord_flip() +
  ggtitle("Figure N: Proportion of Forecasted Votes for Biden By Race")

```
In Figure N show below, we show the projected voting proportions by each state. We see that most of the states are near the 50% mark, with states such as Arizona and Nevada having only around 30% votes for Biden where as Vermont is predicted to have almost 90% of its voters vote for Biden. 

```{r stateproportions, fig.height=8,fig.width=8, fig.cap="Proportion of Biden Votes by state", message=FALSE, echo=FALSE}

state_res %>% 
  ggplot(aes(y = mean, x = forcats::fct_inorder(state), color = "MRP estimate")) + 
  geom_point() +
   
  ylab("Proportion voting for Biden") + 
  xlab("State") + 
  geom_point(data = training_data %>% 
               group_by(state, vote_biden) %>%
               dplyr::summarize(n = n()) %>% 
               group_by(state) %>% 
               mutate(prop = n/sum(n)) %>% 
               filter(vote_biden==1), 
             aes(state, prop, color = "ACS raw data")) +
  scale_color_manual(name = "", values = c("MRP estimate" = "black", "ACS raw data" = "red")) + 
  theme_bw() +
  coord_flip() +
  ggtitle("Figure N: Proportion of Forecasted Votes for Biden By State")


```

Similarly for age, there showed a general trend that the older population are less likely to vote for Trump compared to Biden, with 18-28 years old population projected to vote for Biden almost 70% of the time and just over 45% of the 19 to 58 age group predicted to vote for Biden
```{r ageproportions, fig.height=4,fig.width=8, fig.cap="Proportion of Biden votes within age group.", message=FALSE, echo=FALSE}

age_res %>% 
  ggplot(aes(y = mean, x = forcats::fct_inorder(age_group), color = "MRP estimate")) + 
  geom_point() +
  ylab("Proportion voting for Joe Biden") + 
  xlab("Age") + 
  geom_point(data = training_data %>% 
               group_by(age_group, vote_biden) %>%
               dplyr::summarize(n = n()) %>% 
               group_by(age_group) %>% 
               mutate(prop = n/sum(n)) %>% 
               filter(vote_biden==1), 
             aes(age_group, prop, color = "ACS raw data")) +
  scale_color_manual(name = "", values = c("MRP estimate" = "black", "ACS raw data" = "red")) + 
  theme_bw(base_size = 14) +
  coord_flip() +
  ggtitle("Figure N: Proportion of Forecasted Votes for Biden By Age group")

```

Finally in Figures N and N below, we show that Females are more likely to vote for Biden compared to Males, and a larger portion of individuals with at least some college education are projected to vote for Biden in comparison to High School educated and below.


```{r}
gender_res %>% 
  ggplot(aes(y = mean, x = forcats::fct_inorder(gender), color = "MRP estimate")) + 
  geom_point() +
   
  ylab("Proportion voting for Biden") + 
  xlab("Gender") + 
  geom_point(data = training_data %>% 
               group_by(gender, vote_biden) %>%
               dplyr::summarize(n = n()) %>% 
               group_by(gender) %>% 
               mutate(prop = n/sum(n)) %>% 
               filter(vote_biden==1), 
             aes(gender, prop, color = "ACS raw data")) +
  scale_color_manual(name = "", values = c("MRP estimate" = "black", "ACS raw data" = "red")) + 
  theme_bw(base_size = 14) +
  coord_flip()
```

```{r}
# education_res$age_group <- as.factor(education_res$age_group)

education_res %>% 
  ggplot(aes(y = mean, x = forcats::fct_inorder(education), color = "MRP estimate")) + 
  geom_point() +
  ylab("Proportion voting for Biden") + 
  xlab("Education") + 
  geom_point(data = training_data %>% 
               group_by(education, vote_biden) %>%
               dplyr::summarize(n = n()) %>% 
               group_by(education) %>% 
               mutate(prop = n/sum(n)) %>% 
               filter(vote_biden==1), 
             aes(education, prop, color = "ACS raw data")) +
  scale_color_manual(name = "", values = c("MRP estimate" = "black", "ACS raw data" = "red")) + 
  theme_bw(base_size = 14) 
```

For the US 2020 election between Biden and Trump, we forecast Biden to win the popular vote by 54% to 46% of the votes. 

```{r}

overall_prop <- readRDS('../../processed_data/predictions.RDS')
biden <-sum(overall_prop$vote_weight)
trump <- 1 - biden

Candidates <- c("Biden", "Trump")
Percentage <- c(biden, trump)

data.frame(Candidates, Percentage) %>% 
  ggplot(aes(x = Candidates, 
             y= Percentage)) +
  geom_bar(stat = "identity")
```


# Discussion

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

# Appendix {-}

\newpage


# References
R
Survey and ACS dataset
https://www.voterstudygroup.org/uploads/reports/Data/Nationscape-User-Guide_2020sep10.pdf
https://www.voterstudygroup.org/uploads/reports/Data/NS-Methodology-Representativeness-Assessment.pdf
Wu
https://www.pewresearch.org/fact-tank/2020/10/26/what-the-2020-electorate-looks-like-by-party-race-and-ethnicity-age-education-and-religion/

Model Stuff:
https://www150.statcan.gc.ca/n1/pub/75-001-x/2012001/article/11629-eng.htm

ACS stuff:
https://www2.census.gov/geo/pdfs/education/Uhl_CAS_2011.pdf
https://www.census.gov/programs-surveys/acs/technical-documentation/errata/120.html
https://usa.ipums.org/usa/resources/voliii/formACS2018.pdf
https://usa.ipums.org/usa/resources/codebooks/ACS_codebook.pdf
https://www.politifact.com/factchecks/2014/jan/09/us-census-bureau/americans-must-answer-us-census-bureau-survey-law-/

Tausanovitch, Chris and Lynn Vavreck. 2020. Democracy Fund + UCLA Nationscape, October 10-17, 2019 (version 20200814). Retrieved from [URL].
