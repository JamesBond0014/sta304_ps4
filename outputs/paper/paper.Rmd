---
title: "Biden Expected to Win 54% of Popular Vote in 2020 US Presidential Election"
author: "James Bao, Zakir Chaudry, Alan Chen, Xinyi Zhang"
date: "`r format(Sys.time(), '%d %B %Y')`"
thanks: "Code and data supporting this analysis are available at: https://github.com/JamesBond0014/sta304_ps4."
abstract: |
  | After Trump's shocking upset in the 2016 US Presidential election, political pundits (and American in general) have been closely monitoring the 2020 US presidential election which will determine the leader of the free world for the next four years. In this paper we developed a multi-level regression model with post-stratification by training a model using voter survey results and predicting the outcome of the popular vote using large-scale demographic data for the American population. Our model predicts that Joe Biden will win the 2020 Presidential election with a 53.5% of the popular vote and we established that gender and education are the most significant variables when forecasting voting. Our prediction as well as our breakdown of votes by demographic group could potentially provide the candidates of the election with information on how to target voter.
  |
  | **Keywords:** forecasting, US 2020 Election, Trump, Biden, multilevel regression with post-stratification

output:
  bookdown::pdf_document2:
toc: FALSE
bibliography: references.bib
nocite: '@*'
link-citations: yes
---

```{r setup, include=FALSE}
library(haven)
library(tidyverse)
library(knitr)
```
```{r loading, echo=FALSE}
# Load required data and models
cleaned_data <- data.frame(read.csv(file = "../../inputs/cleaned_acs.csv"))
```

# Introduction

Unless you've been living under a rock, the question in everyone minds is who will the 2020 US presidential election. Polls are consistently being updated, attempting to reflect the newest results, as candidates attempts to sway various groups in USA in order to gain more votes. By choosing 5 explanatory variables composed of: age (by groups of 10 years), gender, state, race and education we fit an logistic regression model with multilevel regression with post-stratification in order to predict the outcome of this highly anticipated election. 

Throughout our analysis, we found relationships between various demographic information and a persons liklihood to vote for either Trump or Biden, we analyzed the significance of our model, and the importance of using multilevel regression with post-stratification. We found that we were able to make predictions with approximately 63% accuracy, and only certain values within our variables are strong indicators of a person's forecasted vote. For examples, 90% of the Vermont population is projected to vote for Joe Biden, and approximately 90% of Black or African Americans are likely to vote for Biden. However, not many variables provided strong support for Donald Trump, noticeably 70% of the state of Arizona is likely to vote for Trump. However, individually, many of our variables are close to 50% and do not provide a strong indication on an individuals voting intention. 

This paper discusses the 2 datasets we used, how they were collect and key highlights of tehse datasets, followed by visualizations, explanations of our results, and how we used multilevel regression with post-stratification to build a logistic regression model with a small dataset from Nationscape in order to predict the voting results on a much larger dataset provided by American Community Survey which can represent the population. 

# Data
To train our model to predict the outcome of the 2020 US presidential election, we used Wave 49 of the Nationscape Dataset (results from the week of June 18-24, 2020). We will discuss how this data was collected, its key features, and what the data looks like in the section titled "Individual-level survey dataset."

To make predictions on the outcome of the 2020 US presidential election, we used the results of the 2018 American Community Survey. We will discuss how this data was collected, its key features, and what the data looks like in the section titled "Post-stratification dataset." The explanation of multilevel modelling with post-stratification can be found in the "Model" section.

## Individual-level survey dataset

### Data collection
The Nationscape Project is 16-month-long voter study (from July 2019 to January 2021) that conducts weekly surveys regarding the 2020 US presidential election. We will mainly discuss Wave 49 of the Nationscape Dataset for the reminder of this paper.

From June 18, 2020 to June 24, 2020, Nationscape collected data on public opinion about the 2020 presidential campaign and election by conducting 15-minute online interviews. Their target is the American "population." Unfortunately, the published information on their methodology is not more specific as to what constitutes a member of the American population. Presumably (based on analyzing the data), their target population is all adult individuals presently residing in the United States. 

Nationscape used the audience of market research platform Lucid as its sampling frame. Sampling frames are lists of the units (individuals in our case) that will be selected for the survey sample, meaning that the survey respondents on Lucid form a list of a subset of the target population (from which a sample will be taken). Finally, a sample matching the demographics of the American population is selected from the frame using a purposive sampling method. This is a non-probability sampling method where the researcher decides which samples are most representative of the target population. More specific information about their sampling method was not provided (besides a statement that the sampling was not random). After being contacted by Lucid to take the survey, respondents are immediately redirected to Nationscape survey software where the questionnaire starts. 

Nationscape reported that the nonresponse rate was about 17%. Another 8% of responses were removed for speeding (spending less than 6 minutes completing the survey) or for "straight-lining" answers (selecting the same response for all policy questions) resulting in a final sample size of 6,532 respondents. To reduce the effects of non-response bias and to ensure results were representative of the US population, survey responses were weighted using data from the 2018 American Community Survey (for demographic variables) and from the United States Elections Project and MIT Election Lab (for 2016 vote). This ensures that the discrepancy between the target population and survey responses is minimized. Lastly, Nationscape assessed the representativeness of the survey sample by including questions from the 2018 Pew evaluation of non-probability samples and comparing their results to Pew findings and government benchmarks. Overall, the difference between Nationscape results and government benchmarks was comparable to the difference between Pew findings and government benchmarks. Consequently, Nationscape concluded that estimates from their dataset should be considered sufficiently valid (at least in comparison to other political polling non-probability samples analyzed by Pew).

The strengths of Nationscape's survey methodology include pilot testing their questionnaire for several weeks, which allowed staff to finetune survey questions and respondent selection criteria. Along the same line, the survey strikes a good balance between being detailed enough to capture useful data while being short enough to hold respondent attention. Furthermore, the high frequency of the data collection process provides the dataset with a week by week breakdown of voter sentiment, potentially capturing changes in public political opinion as news or controversies break. Lastly, the response rate is extremely good for an online survey, indicating that the vast majority of the selected sample responded. In fact, a response rate of over 80% is very high and likely due to the distribution of the survey through the Lucid platform (and certain characteristics of or certain incentives for survey respondents on the platform). 

On the other hand, a major weakness of the survey is that sampling was not conducted at random but rather demographic criterias were designed by the Nationscape staff. Another weakness is that the sampling frame is not necessarily representative of the American population (those who aren't members of survey panels or aren't comfortable sharing political opinions are likely not represented). Lastly, the results are likely subjected to response bias because of the subjective nature of the research topic. However, as previously mentioned, Nationscape addressed these weaknesses by comparing their results to the results from 2018 Pew evaluations on non-probability sampling (and found the accuracy and representativeness of their dataset to be comparable). 

### Data features and visualization
The full dataset for Wave 49 (@nationscape) consists of 6,532 responses for over 260 variables. They cover topics ranging from the presidential candidates to government policies, current events, political views and respondent demographics. In the interest of brevity, we will focus our discussion on the explanatory and response variables relevant to our model. We aim to predict the winner of the popular vote in the 2020 US presidential election so our response variable of choice is vote_2020. We chose age, gender, race_ethnicity, state, and education as explanatory variables based on the demographic characteristics that are most important in determining user vote and our ability to match these variables with the post stratification dataset. In greater detail, here are the chosen variables:

- vote_2020: the vote of the respondent given that the Democratic nominee is Joe Biden and the Republican nominee is Donald Trump 

Table 1: Respondent 2020 US presidential election vote distribution
```{r, echo=FALSE}
raw_data <- read_dta("../../inputs/data/ns20200625/ns20200625.dta")
raw_data <- labelled::to_factor(raw_data)

vote_2020 = table(raw_data$vote_2020)
kable(vote_2020, col.names = c("vote_2020", "Frequency"))
```
- age: the age of the respondent in years at the time of the survey

Table 2: Respondent age statistics
```{r, echo=FALSE}
age = as.array(summary(raw_data$age))
kable(age, col.names = c("Statistics", "Values"))
```

- gender: the sex of the respondent (the options being "Male" or "Female")

Table 3: Respondent gender distribution
```{r, echo=FALSE}
gender = table(raw_data$gender)
kable(gender, col.names = c("gender", "Frequency"))
```

- race_ethnicity: the race of the respondent 

Table 4: Respondent race distribution
```{r, echo=FALSE}
race_ethnicity = table(raw_data$race_ethnicity)
kable(race_ethnicity, col.names = c("race_ethnicity", "Frequency"))
```

- state: the state the respondent resides in (table omitted in the interest of space)

- education: the highest level of education completed by the respondent

Table 5: Respondent education distribution
```{r, echo=FALSE}
education = table(raw_data$education)
kable(education, col.names = c("education", "Frequency"))
```

For the variables age, gender, race_ethnicity, state, and education, we did not find similar equivalents in the dataset. We did find that the variable trump_biden (the candidate that the respondent would support if the election was a contest between Donald Trump and Joe Biden) was similar to our selected variable vote_2020. However, as vote_2020 is more representative of the nature of the popular vote, we did not end up choosing trump_biden. 

When cleaning the data, we merged some of the factors of the variables to match the granularity of the data in the post-stratification dataset. This included splitting age responses into bins of size 10, reducing education to two bins ("High School or Less" and "Post Secondary or More"), and combining Asian Indian, Korean, Filipino, Vietnamese, and Pacific Islander ethnicities into "Asian (Other)" (Chinese and Japanese remained their own factors because this is the level of specificity available in the post-stratification dataset). Lastly, we took a subset of the dataset where respondents had decided to vote for either Trump or Biden in the 2020 US presidential election (for the purposes of being able to predict the popular vote using a binary model).

```{r, echo=FALSE}
reduced_data <- 
  raw_data %>% 
  select(vote_2020,
         age,
         gender,
         race_ethnicity,
         state,
         education)

reduced_data <- reduced_data[complete.cases(reduced_data),]

# Remove people under 18 and over 78 
reduced_data <- reduced_data[reduced_data$age <= 78 &
                               reduced_data$age >= 18,]

# Splitting age responses into groups of 10 years
reduced_data$age_group <- cut(reduced_data$age, 
                              breaks = seq(10, 88, 10),
                              labels = c("18 to 28","29 to 38","39 to 48",
                                         "49 to 58","59 to 68","69 to 78", 
                                         "79 and above"), 
                              right=FALSE)


# Combine education options to match with post stratification data
reduced_data$education <- as.character(reduced_data$education)
reduced_data$education[
  grepl("Completed some graduate, but no degree", reduced_data$education)|
  grepl("Masters degree", reduced_data$education)|
  grepl("Doctorate degree", reduced_data$education)|
  grepl("Associate Degree", reduced_data$education)|
  grepl("college", tolower(reduced_data$education))
] <- ('Post Secondary Degree')
reduced_data$education[
  grepl("Grade", reduced_data$education)|
  grepl("high school", tolower(reduced_data$education))
] <- ('High School or Less')
reduced_data$education <- as.factor(reduced_data$education)

# Combine race factors to match with post stratification data
reduced_data$race_ethnicity <- as.character(reduced_data$race_ethnicity)
reduced_data$race_ethnicity[
  reduced_data$race_ethnicity == 'Asian (Asian Indian)' |
    reduced_data$race_ethnicity == 'Asian (Korean)' |
    reduced_data$race_ethnicity == 'Asian (Filipino)' |
    reduced_data$race_ethnicity == 'Asian (Vietnamese)'|
    grepl("Pacific", reduced_data$race_ethnicity)
] <- ('Asian (Other)')
reduced_data$race_ethnicity <- as.factor(reduced_data$race_ethnicity)

# Filtering for only Trump and Biden voters
reduced_data <- reduced_data[reduced_data$vote_2020 == "Donald Trump" |
                             reduced_data$vote_2020 == "Joe Biden",]
# Change vote to binary variable: 1 for voting for Biden, 0 for voting for Trump
reduced_data$vote_biden <- plyr::mapvalues(reduced_data$vote_2020, 
                                           c("Donald Trump", "Joe Biden"),
                                           c(0, 1))
# Drop unused levels
reduced_data <- droplevels(reduced_data)
```

The distributions of each of our cleaned variables (with the exception of gender) are shown in the following few pages. 

```{r agedist, fig.height=4, fig.cap="Distribution of the age of survey respondents in percentages.", message=FALSE, echo=FALSE}
# Total number of observations in the dataset
total_count = nrow(reduced_data)

# Summary table with counts for sex
tibble_age <- data.frame(table(reduced_data$age_group)) %>%
  rename(
    age_group = Var1,
    count = Freq
  )
# Distribution of sex as a bar graph in percentages
ggplot(tibble_age, aes(x = age_group, y = count/total_count)) + 
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(count/total_count*100), "%")),
            color="white",
            position = position_stack(vjust = 0.5)) +
  xlab("Age") +
  ylab("Percentage of Respondents")
```


```{r racedist, fig.height=4, fig.cap="Distribution of the race of survey respondents in percentages.", message=FALSE, echo=FALSE}
# Summary table with counts for race
tibble_race <- data.frame(table(reduced_data$race_ethnicity)) %>%
  rename(
    race = Var1,
    count = Freq
  )
tibble_race$race <- factor(tibble_race$race, 
                                      levels = c("White", "Black, or African American", 
                                                 "Asian (Chinese)", "Asian (Japanese)",
                                                 "Asian (Other)", "American Indian or Alaska Native", 
                                                 "Some other race"))
# Distribution of sex as a bar graph in percentages
ggplot(tibble_race, aes(x = race, y = count/total_count)) + 
  geom_bar(stat = "identity") +
  geom_text(
    aes(x = race, y = count/total_count, label = paste0(round(count/total_count*100), "%")),
    position = position_dodge(width = 1),
    vjust = -0.5, size = 3) +
  theme(axis.text.x = element_text(angle = 20, hjust=1)) +
  xlab("Race") +
  ylab("Percentage of Respondents")
```

```{r educationdist, fig.height=4, fig.cap="Distribution of the education of survey respondents in percentages.", message=FALSE, echo=FALSE}
# Summary table with counts for race
tibble_education <- data.frame(table(reduced_data$education)) %>%
  rename(
    education = Var1,
    count = Freq
  )
# Distribution of sex as a bar graph in percentages
ggplot(tibble_education, aes(x = education, y = count/total_count)) + 
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(count/total_count*100), "%")),
            color="white",
            position = position_stack(vjust = 0.5)) +
  xlab("Education") +
  ylab("Percentage of Respondents")
```

```{r statedist, fig.height=4, fig.cap="Distribution of the state of survey respondents in percentages.", message=FALSE, echo=FALSE}
# Summary table with counts for race
tibble_state <- data.frame(table(reduced_data$state)) %>%
  rename(
    state = Var1,
    count = Freq
  )
# Distribution of sex as a bar graph in percentages
ggplot(tibble_state, aes(x = state, y = count/total_count)) + 
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust=1)) +
  xlab("State") +
  ylab("Percentage of Respondents")
```

```{r votedist, fig.height=4, fig.cap="Distribution of the vote of survey respondents in percentages.", message=FALSE, echo=FALSE}
# Summary table with counts for race
tibble_vote <- data.frame(table(reduced_data$vote_2020)) %>%
  rename(
    vote = Var1,
    count = Freq
  )
# Distribution of sex as a bar graph in percentages
ggplot(tibble_vote, aes(x = vote, y = count/total_count)) + 
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(count/total_count*100), "%")),
            color="white",
            position = position_stack(vjust = 0.5)) +
  xlab("Vote") +
  ylab("Percentage of Respondents")
```

Out of the respondents, approximately 50% were female and 50% were male (hence gender was not graphed). The distribution of respondent age approaches a bell shaped curve with much fewer "18 to 28" year-olds and individuals aged "79 and above" than any other age group (Figure \@ref(fig:agedist)). This is not too surprising for the older age division because their technology use is likely very limited and thus few older respondents are reached. However, it's surprising so few young adults participated in the survey. The majority of survey respondents were White (76%), followed by African American (11%), Asian (4%), Native American (1%), and some other race, meaning bi- or multi-racial (6%) (Figure \@ref(fig:racedist)). This mostly aligns racial demographic estimates from the US Census Bureau although they reported that only 2.8% of the American population is of two or more races while 13% are African American and 6% are Asian. This redistribution of minority frequencies is unfortunate but shouldn't end up significantly impacting our prediction (because our calculation of the popular vote is conducting using post-stratification data while we are only using survey data to train our model on the relationship between our explanatory and response variables).

71% of responses had some level of post secondary education (Figure \@ref(fig:educationdist)), which doesn't line up with US Census estimates of only 32% of Americans having obtained a bachelor's degree or higher (again, emphasizing the need for prediction on post-stratification data). The state distribution has a lot of factors but basic sanity checks suggest that the distribution is proportional to the American population with the majority of respondents residing in California, Florida, New York, and Texas (Figure \@ref(fig:statedist)), which are the four most populous states. Lastly, 52% of respondents planned on voting for Joe Biden with Donald Trump behind by 4 points (Figure \@ref(fig:votedist)). We look forward to predicting the election results by training our model on this dataset and seeing how the voting distribution changes when generating predictions given post-stratification data.



## Post-stratification dataset

### Data collection
The post-stratification dataset was gathered from the American Community Survey (@acsdataset), a project aiming to mitigate issues stemming from the Census’ 10-year intervals by providing an annualized version of data like that produced by the decennial census long form. The dataset used in this study is specifically that of the 2018 ACS data. The ACS data can be accessed at the IPUMS website. More details on attaining and cleaning data are found in 01-data_cleaning-post-strat.R in the scripts folder of the git repository. TODO

The target population, much like the Census, is essentially anyone who resides in a dwelling in the US. Following this, the sampling frame of the ACS is the Master Address File that is maintained by the US Census Bureau. (@cas2011) Created for the 2000 Census, it was originally based on the 1990 Address Control File and the United States Postal Service’s Delivery Sequence File. The maintaining and updating of this file is crucial to the efforts of the ACS and any other body that makes use of it. In addition, the ACS samples 2.5 percent of the population living in “Group Quarters”, which are non-housing units (eg. nursing homes, prisons, college dorms, etc.). In total, the 2018 ACS data contains about 3.2 million observations, sampled from across the country.

Every month, a systemic sample is created for each US county or equivalent, where they are mailed the ACS survey at the start of the month. (@acs_codebook) As of February 2002, the sampling rate of all counties has been 2.5%, except for Houston, Texas, which is sampled at 1% (to reduce cost due to the size of the population). For every site, the sampling is broken into two steps. The first step is sampling 17.5% of the population, which is then subsampled from to achieve the desired 2.5 or 1% of the county. All non-respondents are subsequently contacted by phone for a computer assisted telephone interview one month later. One third of non-respondents that have reached this point are then sampled from to be contacted for a computer assisted personal interview following the previous telephone interview attempt. Beyond this sample (referred to as the National Sample or Supplemental Sample), data was also collected at 31 selected test sites to represent areas with various county population sizes or areas that were difficult to enumerate. The ACS data is weighted in order to ensure reliable and usable estimated regarding the population. 

### Data features and visualization 
The ACS Questionnaire asks questions regarding every inhabitant in the residence it is sent to. However, the most information is required of “Person 1”, the person whose name the residence is owned in, being bought in, or rented in (or any adult, if none of those labels apply). The ACS questionnaire is extremely like that of the US Census Questionnaire, given that it’s meant to be a substitution for it. It was developed after the Census Bureau was provided with various subjects that other federal agencies justified as important, categorizing each subject as “mandatory”, “required”, or “programmatic”, from which the ACS collected data for both the “mandatory” and “required” subjects. This approach illustrates itself in the questionnaire, as every question is answered by an objective fact, whether it’s a checkbox, a number, or a short answer (such as the person’s major). (@acs_questionnaire) This allows little ambiguity in how the question must be answered, and makes each response fairly easy, which is a major strength of the questionnaire. However, depending on the number of people in the residence, the survey can be on the longer side (up to 11 pages). Combined with the fact that it is legally mandated that the response is filled out, with a potential fine of up to $5000, respondents may feel the need to rush through or give a fake response to questions they may not know immediately (such as when the residence was built). (@politifact)  In addition, in the case that there is not one single person responsible for the residence, like in the case where multiple people are renting a room from one landlord who doesn’t live at the residence, then the data of those who are not “Person 1” is not collected, despite the fact that they are on the same standing in the eyes of the survey. However, no one has been prosecuted for not filling out the survey since 1970 and the case outlined in the second point is relatively rare in the total population, so these weaknesses are fairly minor in the grand scheme of things. 

As mentioned above, the variables we used were age (the age of the respondent), gender (what gender they identified as), race/ethnicity of the respondent, state (the state they lived in), and education (what their highest level of education was). We did not find any variables that were similar enough to the ones chosen. However, as the post-stratification data was a different dataset to that of the survey data, we had to transform the post-stratification data to match that, including general cleaning in the sense of converting non-numeric values to numeric, matching spelling/capitalization of certain responses, and constructing an age_groups variable from the ages given. For more information, please see 01-data_cleaning-post-strat.R in the scripts folder of the github repository.  TODO

```{r poststratage, fig.height=4, fig.cap="Distribution of the age of ACS respondents in percentages", echo=FALSE}
cleaned_data <- labelled::to_factor(cleaned_data)
# Plot age_group
perc_age_group <- cleaned_data %>% count(age_group) %>% mutate(perc = n/nrow(cleaned_data))
ggplot(perc_age_group, aes(x = age_group, y = perc)) + 
  geom_bar(stat = "identity") + 
  geom_text(aes(label = paste0(round(perc*100), "%")), 
            color="white", 
            position = position_stack(vjust = 0.5)) +
  labs(x = "Age", y = "Percentage of Respondents")
```

```{r poststratgender, fig.height=4, fig.cap="Distribution of the gender of ACS respondents in percentages", echo=FALSE}
# Plot Gender
perc_gender <- cleaned_data %>% count(gender) %>% mutate(perc = n/nrow(cleaned_data))
ggplot(perc_gender, aes(x = gender, y = perc)) + 
  geom_bar(stat = "identity") + 
  geom_text(aes(label = paste0(round(perc*100), "%")), 
            color="white", 
            position = position_stack(vjust = 0.5)) +
  labs(x = "Gender", y = "Percentage of Respondents")
```

```{r poststratrace, fig.height=4, fig.cap="Distribution of the race of ACS respondents in percentages", echo=FALSE}
# Plot race
perc_race <- cleaned_data %>% count(race_ethnicity) %>% mutate(perc = n/nrow(cleaned_data))
perc_race$race_ethnicity <- factor(perc_race$race_ethnicity, 
                                      levels = c("White", "Black, or African American", 
                                                 "Asian (Chinese)", "Asian (Japanese)",
                                                 "Asian (Other)", "American Indian or Alaska Native", 
                                                 "Some other race"))
ggplot(perc_race, aes(x = race_ethnicity, y = perc)) + 
  geom_bar(stat = "identity") +
  geom_text(
    aes(x = race_ethnicity, y = perc, label = paste0(round(perc*100), "%")),
    position = position_dodge(width = 1),
    vjust = -0.5, size = 3) +
  theme(axis.text.x = element_text(angle = 20, hjust=1)) + 
  labs(x = "Race", y = "Percentage of Respondents")
```

```{r poststratedu, fig.height=4, fig.cap="Distribution of the education of ACS respondents in percentages", echo=FALSE}
perc_education <- cleaned_data %>% count(education) %>% mutate(perc = n/nrow(cleaned_data))
ggplot(perc_education, aes(x = education, y = perc)) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = paste0(round(perc*100), "%")), 
            color="white", 
            position = position_stack(vjust = 0.5)) +
  labs(x = "Education", y = "Percentage")

```

```{r poststratstate, fig.height=4, fig.cap="Distribution of the state of ACS respondents in percentages", echo=FALSE}
# Plot state
perc_state <- cleaned_data %>% count(state) %>% mutate(perc = n/nrow(cleaned_data))
perc_state <- labelled::to_factor(perc_state)

ggplot(perc_state, aes(x = state, y = perc)) + 
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, hjust=1)) +
  labs(x = "State", y = "Percentage of Respondents")
```

Due to the nature of our methodology, we are able to directly compare and contrast demographic distributions for the individual survey data and the post-stratification data. The gender distribution of the post-stratification data (Figure \@ref(fig:poststratgender)) is fairly similar to that of the survey distribution (50-50 split. The ACS distribution of age is closer to what we would expect from "18 to 28" year-olds (Figure \@ref(fig:poststratage)), indicating that the Nationscape survey results are skewed towards an older demographic (Figure \@ref(fig:agedist)). Again, there is certain discrepancy between expected race distributions (Figure \@ref(fig:poststratrace)) and the estimates reported by the Census, which we may take into account when making predictions on the post-stratification dataset as a proxy for the popular vote. 

There is a major difference in education distribution across surveys as shown in Figure \@ref(fig:poststratedu) where only 53% of respondents had some level of post secondary education compared to 71% in Figure \@ref(fig:educationdist). This better matches the data reported in the US, indicating that the true level of post secondary education is much lower than represented in the Nationscape survey (Figure \@ref(fig:educationdist)). Lastly, the state distribution looks proportional to the American population (Figure \@ref(fig:poststratstate)) after conducting the same sanity check as before. The discrepancy between the Nationscape survey data and the post-stratification data with regards to age as well as education indicates how important it is that we are using multi-level regression model with post-stratification to predict the popular vote. 

# Model
The purpose of our model is to predict a person's vote in the 2020 US election given their demographic characteristics. The dependent variable is a binary categorical variable, where 0 represents voting for Trump and 1 represents voting for Biden. The independent variables are the characteristics of the person (age, gender, race, state, and education). To avoid multicollinearity (when there is a relationship between the independent variables) which causes unreliable regression estimates, a few carefully selected independent variables were used for our model. One criteria for the selection was that the variables had to be both present in the Nationscapes dataset and the ACS dataset, so that they be included in post-stratification. Another criteria is that the characteristic should group people that share a similar perspective which may impact on their voting decision. Through literature research (@statscan), we found that some helpful indicators were age, gender, race, state, education, and self-reported income. To finalize the selection, characteristics should not be highly correlated and the data provided should make logical sense. Since the data quality for income was generally poor and it worsened the model performance, income was ultimately excluded from the model. Therefore, the predictors used for the model are age, gender, race, state, and education.

We found when exploring and graphing the data in the section above that the survey data has underrepresented or overrepresented certain characteristics compared to the actual distributions in the population (namely younger individuals were underrepresented and individuals with higher education were overrepresented in the Nationscape survey data). Therefore, if we were to predict the outcome of the popular vote using the demographic proportions found in the Nationscape survey, we would find that the prediction is biased because the survey sample is not representative of the American population. Hence, we use multi-level regression with post-stratification (MRP) to adjust the influence of each subgroup of the respondents to get a better match on the actual population distribution. To do this, a multi-level model is required as well as another resource (ACS dataset in our case) for post-stratification.

## Multi-level logistic regression model
Since we have represented predicting the popular vote as a binary classification problem using multiple explanatory variables, binary logistic regression is a suitable choice for the model. We use logistic regression to predict the probability of a person voting for Biden and determine their vote by rounding to the nearest represented binary categorical variable. Logistic regression takes independent variables (in our case age, gender, race, state, and education, state, and race) as inputs. Based on the assigned weights and a logit function, the output will be a probability [0,1] of voting for Biden/Trump. Equation \@ref(eq:lrm) is the equation of the logistic regression model and this defines the multi-level regression part of MRP:

Equation 1: Logistic regression model
\begin{equation}
Pr(Y_i \in \{Trump, Biden\}) = logit^{-1}(\alpha_{a[i]}^{\textrm{age}} + \alpha_{g[i]}^{\textrm{gender}} + \alpha_{e[i]}^{\textrm{edu}} + \alpha_{s[i]}^{\textrm{state}} + \alpha_{r[i]}^{\textrm{race}}) (\#eq:lrm)
\end{equation}

where $Y_i$ represents the probability a respondent is likely to vote for Trump or Biden given various demographic information and the $\alpha$'s are age, gender, education, state, and race and the notation $\alpha_{a[i]}$ refers to the age group the i-th individual belongs to, $\alpha_{g[i]}$ refers to the gender group the i-th individual belongs to, and so forth.

The Nationscape dataset contains voter characteristics and their expected vote for the 2020 US Presidential election, so the model was trained using cross-validation on that dataset. 95% of the dataset was used as the training set to determine the best weights for the model and the remaining 5% of the dataset as the test set to verify the accuracy of the model. The vast majority of the data is used for the training set because we wanted to ensure that the data does not overfit due to a small sample size. 

Furthermore, there were many adjustments that had to be done with the inputs to improve model accuracy. Apart from self-reported income, education was another independent variable that we debated whether it should be included in the model. Education severely impacted our cross-validation accuracy due to a wide range of values which were too specific for our purpose. These issues were resolved by summarizing all education levels into two levels: "High School or Less" and "Post Secondary or More". We were able to confirm that there was a significant difference in the voting preference between the two groups, consequently helping our model  perform better.

Another variable that initially caused inaccurate results was age. There was no clear trend in having age as a numeric continuous value for predicting votes. As a result, the numerical age values were grouped into bins of size 10 (eg. 18-28, 29-38) in order to better represent different age groups. Another grouping we tried was splitting the data into youth, middle age, and seniors (18-35, 36-55, 55+); however, that split was not able to capture any significant voting pattern in the age groups because we found that the model accuracy actually worsened. So grouping age by bins of size 10 gave us the best results and is the configuration that was included in the model.

After finalizing the inputs that went into our model, we implemented the logistic regression model using the BRMS library written in the programming language R (@citeR). We ran our script using the software RStudio.  We did not run into any diagnostic issues when running our model. As briefly mentioned, we conducted cross-validation to check our model. The results are fittingly covered in the Results section.

## Post-stratification
As we have previously established, we need to use post-stratification to correct our model estimate of the popular vote because of the discrepancy between the demographic distribution of and the demographics of the American population. We conduct post-stratification by calculating a weighted average of estimates for all combinations of explanatory variables. We found the weights to do so from the ACS dataset. However, we cannot immediately use the ACS distributions as the dataset may not represent American demographics accurately. For example, as we found in our data exploration, there is a discrepancy between the minority distribution found in the ACS dataset and the data reported by the US Census. If minorities tend to democratically and for Biden, we would have a bias against Biden because they are underrepresented in the ACS dataset. 

When we perform post-stratification on the ACS data, we find all combinations of our variables and find the weight of each combination representation within the US. Using the PERWT variable provided by the ACS, we can calculate how much of the population each combination represents within the United States. As per the IMPUMS webpage, "PERWT indicates how many persons in the U.S. population are represented by a given person in an IPUMS sample", meaning we can find all the individuals with the same values for the combination of variables we are measuring and add their PERWT values together to estimate how much these specific values would weigh.

Using MRP provides many benefits. As mentioned, we can more accurately estimate the weight of our sample predictions in relation to the population without being heavily affected by any sampling bias. It also allows us to use a small sample for training, and apply the model to a much larger sample that better represents the population. In our example, we can use a  small scale election survey from Nationscape and train a model which allows us to predict election results for the respondents in the American Community Survey consisting of over 3 million data. This is crucial, as collecting surveys with a useful sample size with regards to an upcoming election can be extremely expensive and time consuming, as these poll results may be time sensitive. The option to collect from a small sample size allows statisticians to save money and time while providing significant results about the larger population by applying their surveys to general census information.  Another noted benefit is computational efficiency. Instead of individually predicting over 3 million people, we summarize all the comparisons and only predict on over 8000 individuals and multiply their proportions, which allows us to compute our predictions significantly faster. 

However, there are also cons to using MRP. For instance, we are limited to only use variables that will be found in general census data. If we are interested in looking at religion but the post-stratification dataset does not have that field, we are unable to include religion in our model. This can be challenging as information such as an individual's vote in 2016 may be extremely important as a variable, but since the ACS dataset does not contain such information, we are use it to our advantage. Furthermore, if a variable is broken down to a different granularity, we must group by the common group. For example, if dataset 1 has age grouped in bins of size 3 and dataset 2 has age grouped in bins of 5 (eg. 5-8,9-12 vs 5-10,15-20) then we must map them into bins that are multiples of 15. We cannot use groups of 3, because we cannot accurately break down groups of 5 into groups of 3. Therefore we lose granularity in situations where it may be desired.

In this case, using MRP makes sense as we have voting information provided by Nationscape, however this dataset is not distributionally representative of the target population. We also have access to the ACS which contains a lot of distributional data about the American population but it does not contain any information on who an individual will vote for in the upcoming election. Using MRP allows us to use both datasets in order to predict the outcome of the popular vote of the 2020 US presidential election.


# Results
We will go over our cross validation results, the explanatory variables that have the largest impact on the response variable, and post-stratification predictions.

```{r crossval, fig.height=4, fig.cap="Cross-validation results", echo=FALSE}
model_education <- readRDS("../../model/4chains_3000iter_.rds")
reduced_data <- readRDS("../../inputs/training_data.Rda")
total_rows <- nrow(reduced_data)
# get updated columns (used for the model)
reduced_data <- reduced_data %>% select(vote_biden,age_group, education, gender,
                                        race_ethnicity, state)
set.seed(50)
#set
shuffle_indices <- sample(total_rows)
data <- reduced_data[shuffle_indices,]
boundary <- as.integer(total_rows*0.95)
training <- data[0:boundary,]
testing <- data[boundary:total_rows,]
#testing the accuracy of the model
probability <- predict(model_education, type = "response", newdata = testing)
probability <- if_else(probability[,1] >0.5,1,0)
testing$probs <- probability
testing <- testing %>% mutate(acc = probs==vote_biden)

data.frame(table(testing$acc)) %>% 
  ggplot(aes(x = Var1, y = Freq/(sum(Freq)))) +
  geom_bar(stat="identity") +
  xlab("Prediction") +
  ylab("Percentage") + 
  geom_text(aes(label = paste0(round(Freq/(sum(Freq))*100), "%")), 
            color="white", 
            position = position_stack(vjust = 0.5))
```


```{r results, fig.height=4, fig.cap="Prediction distribution after cross validation", echo=FALSE}
wrong <- as.numeric(testing$vote_biden) - as.numeric(testing$probs) - 1
wrong <- plyr::mapvalues(wrong, c(-1,0,1), c("False Positive", 
                                                   "Correct",
                                                   "False Negative"))
data.frame(table(wrong)) %>% 
  ggplot(aes(x = wrong, y = Freq/sum(Freq)))+
  geom_bar(stat="identity") +
  xlab("Prediction Type") +
  ylab("Percentage")+ 
  geom_text(aes(label = paste0(round(Freq/(sum(Freq))*100), "%")), 
            color="white", 
            position = position_stack(vjust = 0.5))
```

Overall, our model performs non-trivially with a cross-validation accuracy of approximately 63% (Figure \@ref(fig:crossval)). This is significantly better than a random guess (which would have an accuracy of 50%). Furthermore, by investigating the source of our inaccuracies (Figure \@ref(fig:results)), we see that of the approximately 21% of our predictions were false negatives (Predicted Trump vote when it should predict Biden) and 15% of our predictions were false positives (predicted Biden when it should have predicted Trump vote). This was significantly better than some of the other models we tried as discussed in the previously, as we had a large number of false negatives in many models when education was split into multiple groups rather than just College or no College.  


```{r, echo=FALSE}
coefficient <- matrix(c(1.40, 0.44, 0.53, 2.26, 1.39, 0.71, 0.07, 2.89, 0.75, 0.34, 0.08, 1.42, 2.38, 0.32, 1.75, 3.01, 
                        0.92, 0.32, 0.31, 1.54, 0.13, 0.29, -0.42, 0.70, -0.44, 0.06, -0.57, -0.32, -0.27, 0.21, -0.68, 0.13, -0.77, 0.20,-1.17,-0.38, -1.02,0.20,-1.42,-0.64,-0.94,0.21,-1.35,-0.55, -0.83,0.20,-1.25,-0.45, -0.74,0.22,-1.19,-0.32, 0.36,0.07,0.22,0.51), ncol=4, byrow=TRUE)
colnames(coefficient) <- c("Estimate", "Estimate Error", "l-95% CI", "u-95% CI")
rownames(coefficient) <- c("race_AsianChinese", "race_AsianJapanese", "race_AsianOther", "race_AfricanAmerican", "race_Someotherrace", "race_ethnicityWhite", "genderMale", "age_group29to38", "age_group39to48", "age_group49to58", "age_group59to68", "age_group69to78", "age_group79andabove", "education_PostSecondaryDegree")
coefficient <- as.table(coefficient)
kable(coefficient)
```
Table 6: Coefficients estimates and coefficient intervals given a confidence level of 95%

We consider the strength of our variables by looking at the coefficient estimates and how large the interval of the estimate is at a 95% confidence level (we calculate this by subtracting "u-95% CI" which stands for upper 95% confidence interval and "l-95% CI" which stands for lower 95% confidence interval). The output for the states are omitted in the interest of space (and because we noticed that the interval of their estimate is very large at a 95% confidence level is very large with absolute value of ~4 for each state, indicating that the estimate for each state is uncertain and thus state is not a strong predictor of voting). Although all variables listed in Table 6 have a smaller interval that state, a few variables stand out has having especially small confidence intervals. They are gender (interval of only 0.25) and education (interval of only 0.29). This indicates that the model is fairly certain in its estimate of the coefficients for gender and education. Therefore, these two variables likely have the strongest influence on the response variable because there is variability in the possible value of their coefficients at a confidence level of 0.95. Furthermore, the low error of these two variables reemphasizes our conclusion that age and education are the strongest determiners of 2020 voting. The other variables are stronger predictors than state but not as strong as age and education.

```{r, echo=FALSE}
cell_counts <- readRDS("../../inputs/cell_counts.RDS")
training_data <- readRDS("../../inputs/training_data.RDA")
model <- readRDS("../../model/4chains_3000iter_.rds")

race_res <- readRDS("../../processed_data/race_res.RDS")
gender_res <- readRDS("../../processed_data/gender_res.RDS")
state_res <- readRDS("../../processed_data/state_res.RDS")
age_res <- readRDS("../../processed_data/age_res.RDS")
education_res <- readRDS("../../processed_data/education_res.RDS")

```

The post-stratification data of the 2018 ACS data showed that most of the variables had very little biases since the MRP estimates were very close to the original 2018 ACS data. In Figure \@ref(fig:raceproportions) shown below, we see that our raw data proportions was very close to the estimate proportions. Furthermore, this figure forecasts that almost 90% Black, or African American voters will vote for Joe Biden, 75% to 80% of Chinese and Japanese voters are predicted to vote for Biden, while approximately only 45% of white and American Indian or Alaska Native voters are projected to Vote for Joe Biden instead of Donald Trump. Overall, ethnic minorities are projected to vote for Biden over Trump.

```{r raceproportions, fig.height=4, fig.cap="Proportion of forecasted votes for Biden by race", message=FALSE, echo=FALSE}
# following graphs are inspired by Monica Alexander 
# https://github.com/MJAlexander/marriage-name-change/blob/master/mncs_mrp.R
race_res %>% 
  ggplot(aes(y = mean, x = forcats::fct_inorder(race_ethnicity), 
             color = "MRP estimate")) + 
  geom_point() +
  ylab("Proportion voting for Biden") + 
  xlab("Race") + 
  geom_point(data = training_data %>% 
               group_by(race_ethnicity, vote_biden) %>%
               dplyr::summarize(n = n()) %>% 
               group_by(race_ethnicity) %>% 
               mutate(prop = n/sum(n)) %>% 
               filter(vote_biden==1), 
             aes(race_ethnicity, prop, color = "ACS raw data")) +
  scale_color_manual(name = "", values = c("MRP estimate" = "black", 
                                           "ACS raw data" = "red")) + 
  theme_bw(base_size = 14) + coord_flip()

```

In Figure \@ref(fig:stateproportions) show below, we show the projected voting proportions by each state. We see that most of the states are near the 50% mark, with states such as Arizona and Nevada having only around 30% votes for Biden where as Vermont is predicted to have almost 90% of its voters vote for Biden. 

```{r stateproportions, fig.height=8,fig.width=8, fig.cap="Proportion of forecasted votes for Biden by state", message=FALSE, echo=FALSE}

state_res %>% 
  ggplot(aes(y = mean, x = forcats::fct_inorder(state), color = "MRP estimate")) + 
  geom_point() +
   
  ylab("Proportion voting for Biden") + 
  xlab("State") + 
  geom_point(data = training_data %>% 
               group_by(state, vote_biden) %>%
               dplyr::summarize(n = n()) %>% 
               group_by(state) %>% 
               mutate(prop = n/sum(n)) %>% 
               filter(vote_biden==1), 
             aes(state, prop, color = "ACS raw data")) +
  scale_color_manual(name = "", values = c("MRP estimate" = "black", "ACS raw data" = "red")) + 
  theme_bw() +
  coord_flip()
```

Similarly for age (Figure \@ref(fig:ageproportions)), there showed a general trend that the older population are less likely to vote for Trump compared to Biden, with 18-28 years old population projected to vote for Biden almost 70% of the time and just over 45% of the 19 to 58 age group predicted to vote for Biden.

```{r ageproportions, fig.height=4,fig.width=8, fig.cap="Proportion of forecasted votes for Biden by age group.", message=FALSE, echo=FALSE}

age_res %>% 
  ggplot(aes(y = mean, x = forcats::fct_inorder(age_group), color = "MRP estimate")) + 
  geom_point() +
  ylab("Proportion voting for Joe Biden") + 
  xlab("Age") + 
  geom_point(data = training_data %>% 
               group_by(age_group, vote_biden) %>%
               dplyr::summarize(n = n()) %>% 
               group_by(age_group) %>% 
               mutate(prop = n/sum(n)) %>% 
               filter(vote_biden==1), 
             aes(age_group, prop, color = "ACS raw data")) +
  scale_color_manual(name = "", values = c("MRP estimate" = "black", "ACS raw data" = "red")) + 
  theme_bw(base_size = 14) +
  coord_flip()

```

Finally in Figures \@ref(fig:genderprop) and \@ref(fig:ageproportions) below, we show that Females are more likely to vote for Biden compared to Males, and a larger portion of individuals with at least some college education are projected to vote for Biden in comparison to High School educated and below.


```{r genderprop, fig.height=4,fig.width=8, fig.cap="Proportion of forecasted votes for Biden by gender.", message=FALSE, echo=FALSE}
gender_res %>% 
  ggplot(aes(y = mean, x = forcats::fct_inorder(gender), color = "MRP estimate")) + 
  geom_point() +
   
  ylab("Proportion voting for Biden") + 
  xlab("Gender") + 
  geom_point(data = training_data %>% 
               group_by(gender, vote_biden) %>%
               dplyr::summarize(n = n()) %>% 
               group_by(gender) %>% 
               mutate(prop = n/sum(n)) %>% 
               filter(vote_biden==1), 
             aes(gender, prop, color = "ACS raw data")) +
  scale_color_manual(name = "", values = c("MRP estimate" = "black", "ACS raw data" = "red")) + 
  theme_bw(base_size = 14) +
  coord_flip()
```


```{r educationprop, fig.height=4,fig.width=8, fig.cap="Proportion of forecasted votes for Biden by education.", message=FALSE, echo=FALSE}
# education_res$age_group <- as.factor(education_res$age_group)

education_res %>% 
  ggplot(aes(y = mean, x = forcats::fct_inorder(education), color = "MRP estimate")) + 
  geom_point() +
  ylab("Proportion voting for Biden") + 
  xlab("Education") + 
  geom_point(data = training_data %>% 
               group_by(education, vote_biden) %>%
               dplyr::summarize(n = n()) %>% 
               group_by(education) %>% 
               mutate(prop = n/sum(n)) %>% 
               filter(vote_biden==1), 
             aes(education, prop, color = "ACS raw data")) +
  scale_color_manual(name = "", values = c("MRP estimate" = "black", "ACS raw data" = "red")) + 
  theme_bw(base_size = 14) 
```

For the US 2020 election between Biden and Trump, we forecast Biden to win the popular vote by 54% to 46% of the votes \@ref(fig:resultspop). 

```{r resultspop, fig.height=4,fig.width=8, fig.cap="Forecasted popular vote results.", message=FALSE, echo=FALSE}

overall_prop <- readRDS('../../processed_data/predictions.RDS')
biden <-sum(overall_prop$vote_weight)
trump <- 1 - biden

Candidates <- c("Biden", "Trump")
Percentage <- c(biden, trump)

data.frame(Candidates, Percentage) %>% 
  ggplot(aes(x = Candidates, 
             y= Percentage)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(Percentage*100), "%")),
            color="white",
            position = position_stack(vjust = 0.5))
```


# Discussion
Figure \@ref(fig:ageproportions) features the trend of the younger voters (between the ages of 18 and 38) to vote left-wing. There are many different reasons this can be the case. Younger people generally tend to be left wing (pew 1). In addition, given the current state of the economy, combined with the fact that in general millennials and Gen-Xers are poorer than their parents, many younger voters are in disillusion with the current government, leading to vote against the Republican Party (@income_inequality). 

Regardless of the reason most young voters support Biden, it’s imperative to realize the repercussions of this. Relative to the 2016 election, young voters are coming out (metaphorically) in droves to vote, be it due to their own stronger opinions, voting drives and increased voting awareness online, or any other facet of their lives. In Texas alone, 1.3 million voters under 30 have already cast their ballot, surpassing the TOTAL number of young votes from 2016 (@young_vote). This can have huge effects nationally, creating a greater likelihood of a Biden victory. 

Contrasting this are the older populations. The majority of voters ages 49 and above support Trump. However, a key point of note is that the difference between Biden support and Trump support is much lower than with younger voters. Given this, there’s not as much change in the way of Trump. That said, in typical elections this does indeed make a huge difference regardless, as it’s the older voters that vote the most (@older_voting). That’s why it’s so important that many young voters are actually voting compared to before, as this drive to vote acts a form of mitigation against the older voters’ support of Trump.

Figure \@ref(fig:genderprop)’s comparison of the votes by gender reveals some interesting findings. While the majority of male voters support Trump, the real highlight of this figure is the fact that nearly 60% of female voters support Biden, essentially twice the difference between Biden and Trump for male voters. Again, there’s many reasons why this could be the case, but there are two major reasons that present themselves. 

The first is the way that Trump has treated women in the past, a notable example being his quote “Grab ‘em by the pussy” (@trump_transcript). The other reason that stands out is the Republican Party’s stance against abortion, an important women’s right issue. This has only been further exemplified with the recent nomination and appointment of Amy Coney to the supreme court, who maintains a personal belief that abortion is immoral and clerked for Justice Scalia, who was a major critic of Roe v. Wade, the case that set the legal precedent for abortion (@healthcare). 


Overall, while there in majority men vote support Trump, a greater percentage of women support Biden. In conjunction with the metric that women vote more than man, this finding supports a Biden victory. (@politics_women)


As discussed in the results, race can also play a significant factor in predicting an individuals vote in the 2020 election. As shown in Figure \@ref(fig:raceproportions), 90% of African American or Black voters are projected to vote for Biden, with Asian voters follow closely with almost 80% of Chinese voters projected to vote for Biden. Given Trump's response to the Black Life's Matter movement, and his response to Coronavirus (China virus according to him), it seems clear that voters belonging to these groups feel strongly against him. There has been protests and outrage country wide in response to police brutality leading to the deaths of black or African American citizens, many protesters feel President Trump is ignoring their voice, focusing on the riots and ignoring the multitude of peaceful protests also happening, going as far as implying the movement is a "symbol of hate (Liptak & Holmes, 2020)." The same article brings up other acts of Trump, such as re-tweeting videos of supporter chanting "White power", Trump diminishing the notion of Black Lives Matter by correcting it into All Lives Matter,  and defending an individual who was accused and convicted of shooting and killing 2 black lives matter protesters (Breuninger, 2020). Googling the words Trump and Black Lives Matter results in pages upon pages of news articles, blog posts and tweets of President Trump seemingly repeatedly condemning Black Lives Matter supporters and supporting anti-black lives matter movements. 

As discussed in the results, race can play a significant factor in predicting an individuals vote in the 2020 election. As shown in Figure N, 90% of African American or Black voters are projected to vote for Biden, with Asian voters follow closely with almost 80% of Chinese voters projected to vote for Biden. Given Trump's response to the Black Life's Matter movement, and his response to Coronavirus (China virus according to him), it seems clear that voters belonging to these groups feel strongly against him. There has been protests and outrage country wide in response to police brutality leading to the deaths of black or  African American citizens, many protesters feel President Trump is ignoring their voice, focusing on the riots and ignoring the multitude of peaceful protests also happening, going as far as implying the movement is a "symbol of hate (@trump_blm)." The same article brings up other acts of Trump, such as re-tweeting videos of supporter chanting "White power", Trump diminishing the notion of Black Lives Matter by correcting it into All Lives Matter,  and defending an individual who was accused and convicted of shooting and killing 2 black lives matter protesters (@trump_kyle). Googling the words Trump and Black Lives Matter results in pages upon pages of news articles, blog posts and tweets of President Trump seemingly repeatedly condemning Black Lives Matter supporters and supporting anti-black lives matter movements. Whether or not these articles have taken his words out of context and twisted his words around, it seems clear that Trump is labeled to be "anti-black" and our predictions indicate that these feelings among the Black and African American community will reflect heavily on their votes, with less than 10% projected to be voting for Trump. Another major issue is the global pandemic caused by the SARS-CoV-2 virus (or according to Trump, the China Virus). Asian American's live in fear of xenophobia and other racist attacks, as many interviewed in the New York Times article (@chinese_safety) recounts racist attacks and comments, many calling for Trump to stop targeting China. The Vancouver police has reported a 600% increase in reports of hate crimes against the Asian community, and while this is not in the USA, it's obvious that Asians living in the USA has decided to hedge their support towards Biden. Of course there are many reasons that affect an individual's decision to vote, but these recent and relevant events may be one of the reasons that voters of Asian and Black or African American races are supporting Joe Biden in the upcoming election. In general, all ethnic minorities (with the exception of American Indian or Alaska Native) support Biden more to various degrees.
Whether or not these articles have taken his words out of context and twisted his words around, it seems clear that Trump is labeled to be "anti-black" and our predictions indicate that these feelings among the Black and African American community will reflect heavily on their votes, with less than 10% projected to be voting for Trump. Another major issue is the global pandemic caused by the SARS-CoV-2 virus (or according to Trump, the China Virus). Asian Americans live in fear of xenophobia and other racist attacks, as many interviewed in the New York Times article (Tavernise & Oppel, 2020) recounts racist attacks and comments, many calling for Trump to stop targetting China. The Vancouver police has reported a 600% increase in reports of hate crimes against the Asian community (Gill, 2020), and while this is not in the USA, it's obvious that Asians living in the USA has decided to hedge their support towards Biden. Of course there are many reasons that affect an individual's decision to vote, but these recent and relevant events may be one of the reasons that voters of Asian and Black or African American races are supporting Joe Biden in the upcoming election. In general, all ethnic minorities (with the exception of American Indian or Alaska Native) support Biden more to various degrees.

In contrast, White and American Indian or Alaska Native groups are predicted to vote more for Trump in comparison to Biden. This is interesting they are different from many other minority groups, who strongly prefer Biden rather than trump. In an article by the CNBC written by @native_vote, American Indian or Alaska Natives are considered to be a key demographic as they could help swing votes in as many as 7 states. This could explain the why the votes are predicted to be split between Trump and Biden, as both parties are putting a focus on gaining these votes. Similarily with the White population, as they represent the vast majority of citizens in USA, they will be the target race demographic for both Trump and Biden as they both fight for what is predicted to be a close election race.

## Education


Figure \@ref(fig:educationprop) outlines the difference in voting based on the voter’s level of education. The first thing of note is that there is a slight majority for Trump for those who have not attended university. However, the difference is so small that it’s essentially equal from a top down perspective. In comparison, those who have attended university tend to support Biden. There are a few reasons this could be the case. One is the notion that universities “make” people more left-wing. Whether or not this is truly the case, and if so for what reason, aside, this is a commonly held belief. However, this may not be the sole reason why such a divide exists. In recent years, conservatives have majorly considered higher education to have a negative effect on the country. (@higher_edu) Inferring from this, conservatives are thus less likely to attend university or view university with a particularly positive perspective than their left-wing counterparts. 

The difference between those who support Trump and those who support Biden for voters who have attended university is not insignificant, but in relation to those previously discussed, it may seem inconsequential.  That said, it’s still an important finding if not for the fact that based on Figure 8, we can see that the majority of the population has attended university. As the majority of post-secondary attendees support Biden and it’s relatively equal for those who haven’t, this naturally supports a Biden victory.

Given that our model is only based on a sample of the population, we can say that it directly represents the "small world". It's using that sample we can create models, see relationships between variables, and make observations/findings. However, arguably more important, is the representation of the "large world". Using post-stratification, we are able to apply these findings on a much larger scale. The model very well represents many aspects of the real world population. However, it isn't a perfect match, as it only approximates real life. For example, these findings are well and good and in theory should perform well in the "large world", but we can't say for sure that it's a perfect match until we see it in action.

If we had the ability to allocate a greater budget to polling companies, we assuredly could achieve a better estimate. The key improvement would be to ask more people and ask more questions, such as religion. With a greater number of questions we could subsequently create more cells to work with, and with increasing the range of respondents stratification can occur.


## Weaknesses and next steps

While our model using multilevel regression with post-stratification was able to provide a election result that resembles many other polling predictions (Biden winning popular vote slightly ahead of Trump), our model had many weaknesses that we would like to address and improve in the future. First and foremost, we only predicted the the votes towards either Biden or Trump, and did not consider the possibility that people may vote for other candidates or people may not vote in this election. While justified as the vast majority of votes entered will go towards Biden and Trump, it will be insightful to analyze which demographics are likely to not vote for the two main parties, and which demographics are likely to not vote at all (maybe younger people are more apathetic to voting, or certain minorities always vote). There were also many articles we found indicating variables such as religion and family status could play a key indication of how individuals vote, but due to restrictions of the datasets, we did not have the information available. We also originally planned on creating a time series analysis on the Nationscape dataset in order to gain some insight in the data we used to train our model which we would like to include in the future. Furthermore, we would like to further explore various variables and factors of our model, in order to get a more significant accuracy. 

\newpage

# References
